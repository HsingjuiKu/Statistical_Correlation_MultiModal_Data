{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PzPdpoi3KZhE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Lambda\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import mode\n",
        "from keras.layers import concatenate, Conv1D, Softmax, multiply, Permute, Flatten\n",
        "from keras import Input, Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, LSTM, Dense, Dropout, TimeDistributed, BatchNormalization\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Concatenate, Dense, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from re import U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import *\n",
        "# from keras.layers.core import *\n",
        "from keras.models import *\n",
        "from keras.optimizers import *\n",
        "from keras.backend import sum\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dropout, Dense,Bidirectional,LSTM"
      ],
      "metadata": {
        "id": "v7XUsJ_rPY3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import numpy as np\n",
        "np.random.seed(2)\n",
        "import tensorflow as tf\n",
        "import scipy.io\n",
        "import h5py\n",
        "import keras\n",
        "!pip install hdf5storage\n",
        "import hdf5storage\n",
        "!pip install xlwt\n",
        "import xlwt as xw\n",
        "\n",
        "from keras.layers import *\n",
        "# from keras.layers.core import *\n",
        "from keras.models import *\n",
        "from keras.regularizers import *\n",
        "from keras.optimizers import *\n",
        "from keras.losses import *\n",
        "from keras import metrics\n",
        "from keras import backend as K\n",
        "from keras.backend import sum, mean, max\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClsHK7FdPW6J",
        "outputId": "1e452595-0011-4da9-81cf-8c5eaf4fc256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.10/dist-packages (0.1.19)\n",
            "Requirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.10/dist-packages (from hdf5storage) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hdf5storage) (1.23.5)\n",
            "Requirement already satisfied: xlwt in /usr/local/lib/python3.10/dist-packages (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulOoSx0YK_EJ",
        "outputId": "871c7eda-ea52-4ee0-8e0f-8547d12d29c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Thanks to marc-moreaux on Github page:https://github.com/keras-team/keras/issues/890 who created this beautiful and sufficient function: )\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)"
      ],
      "metadata": {
        "id": "klwtjlxaKZhK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_banet_model():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 8\n",
        "    LSTM1 = LSTM(lstm_units, return_sequences=True)(singleinput) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1 = Dropout(0.5)(LSTM1)\n",
        "    LSTM2 = LSTM(lstm_units, return_sequences=True)(Dropout1)\n",
        "    Dropout2 = Dropout(0.5)(LSTM2)\n",
        "    LSTM3 = LSTM(lstm_units, return_sequences=True)(Dropout2)\n",
        "    Dropout3 = Dropout(0.5)(LSTM3)\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout3])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 66,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44,45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1)(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1)(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1)(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1)(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1)(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1)(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1)(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1)(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1)(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1)(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1)(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1)(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1)(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1)(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1)(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1)(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1)(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1)(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1)(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1)(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1)(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1)(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22\n",
        "                        ], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(BodyNum, activation='tanh')(DATA)\n",
        "    a = Dense(BodyNum, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output = Dense(2, activation='softmax',name='mainoutput')(attentionresult)\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kZ5LinKkKZhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_paper_lstm(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(32, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(32))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "sbExNTUqKZhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_cnn_lstm(input_shape, num_classes, num_filters=64, kernel_size=5, lstm_units=64, dropout_rate=0.5):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1D Convolutional layer\n",
        "    model.add(Conv1D(num_filters, kernel_size, activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # LSTM layers\n",
        "    model.add(LSTM(lstm_units, return_sequences=True))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(lstm_units))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "v4SlTdHBKZhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def best_model():\n",
        "    input_data = Input(shape=(180, 70))\n",
        "\n",
        "    # Separating the inputs using the crop function\n",
        "    input1 = crop(2, 0, 66)(input_data)\n",
        "    input2 = crop(2, 66, 70)(input_data)\n",
        "\n",
        "    # First model\n",
        "    x = Conv1D(32, 31, padding='same')(input1)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv1D(32, 31, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "\n",
        "    x = Conv1D(32, 31, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv1D(32, 31, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "\n",
        "# Second model\n",
        "    x1 = Conv1D(32, 31, padding='same')(input2)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = Conv1D(32, 31, padding='same')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = MaxPooling1D(2)(x1)\n",
        "\n",
        "    x1 = Conv1D(32, 31, padding='same')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = Conv1D(32, 31, padding='same')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "    x1 = MaxPooling1D(2)(x1)\n",
        "\n",
        "# Fusion\n",
        "    x2 = Concatenate(axis=-1)([x, x1])\n",
        "    x2 = Conv1D(64, 1)(x2)\n",
        "    x2 = BatchNormalization()(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "    x2 = MaxPooling1D()(x2)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    out = Dense(2)(x2)\n",
        "\n",
        "    model = Model(inputs=input_data, outputs=out)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "9M4fqR8qKZhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import U\n",
        "def build_banet_model_middle_origin():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SMEGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 8\n",
        "    LSTM1 = LSTM(lstm_units, return_sequences=True)(singleinput) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1 = Dropout(0.5)(LSTM1)\n",
        "    LSTM2 = LSTM(lstm_units, return_sequences=True)(Dropout1)\n",
        "    Dropout2 = Dropout(0.5)(LSTM2)\n",
        "    LSTM3 = LSTM(lstm_units, return_sequences=True)(Dropout2)\n",
        "    Dropout3 = Dropout(0.5)(LSTM3)\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout3])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    singleinput_SEMG = Input(shape=(180, 2,))\n",
        "    lstm_units = 8\n",
        "    LSTM1_SEMG = LSTM(lstm_units, return_sequences=True)(singleinput_SEMG) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1_SEMG = Dropout(0.5)(LSTM1_SEMG)\n",
        "    LSTM2_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout1_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.5)(LSTM2_SEMG)\n",
        "    LSTM3_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout2_SEMG)\n",
        "    Dropout3_SEMG = Dropout(0.5)(LSTM3_SEMG)\n",
        "    TemporalProcessmodel_SEMG = Model(inputs=[singleinput_SEMG], outputs=[Dropout3_SEMG])\n",
        "\n",
        "\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 70,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    lr = crop(2, 66, 67)(inputs)\n",
        "    ll = crop(2, 67, 68)(inputs)\n",
        "    SEMG_lower_back = concatenate([lr, ll], axis=-1)\n",
        "\n",
        "    SEMGout1 = TemporalProcessmodel_SEMG(SEMG_lower_back)\n",
        "\n",
        "    TemporalAttention_SEMG1 = Conv1D(1, 1, strides=1)(SEMGout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG1 = Softmax(axis=-2, name='TemporalAttenSEMG1')(TemporalAttention_SEMG1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout1 = multiply([SEMGout1, TemporalAttention_SEMG1])\n",
        "    SEMGout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout1)\n",
        "    lower_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout1)\n",
        "\n",
        "\n",
        "    ur = crop(2, 68, 69)(inputs)\n",
        "    ul = crop(2, 69, 70)(inputs)\n",
        "    SEMG_upper_back = concatenate([ur, ul], axis=-1)\n",
        "\n",
        "    SEMGout2 = TemporalProcessmodel_SEMG(SEMG_upper_back)\n",
        "\n",
        "    TemporalAttention_SEMG2 = Conv1D(1, 1, strides=1)(SEMGout2) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG2 = Softmax(axis=-2, name='TemporalAttenSEMG2')(TemporalAttention_SEMG2) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout2 = multiply([SEMGout2, TemporalAttention_SEMG2])\n",
        "    SEMGout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout2)\n",
        "    upper_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout2)\n",
        "\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44, 45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1)(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1)(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1)(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1)(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1)(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1)(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1)(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1)(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1)(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1)(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1)(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1)(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1)(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1)(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1)(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1)(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1)(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1)(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1)(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1)(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1)(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1)(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22, lower_back, upper_back\n",
        "                        ], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(24, activation='tanh')(DATA)\n",
        "    a = Dense(24, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output = Dense(2, activation='softmax',name='mainoutput')(attentionresult)\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tf7R7zEajFxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_banet_model_early():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SEMGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    inputs = Input(shape=(180, 70,))\n",
        "    lstm_units = 90\n",
        "    LSTM1 = LSTM(lstm_units, return_sequences=True)(inputs) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1 = Dropout(0.5)(LSTM1)\n",
        "    LSTM2 = LSTM(lstm_units, return_sequences=True)(Dropout1)\n",
        "    Dropout2 = Dropout(0.5)(LSTM2)\n",
        "    LSTM3 = LSTM(lstm_units, return_sequences=True)(Dropout2)\n",
        "    Dropout3 = Dropout(0.5)(LSTM3)\n",
        "    Conv1D1 = Conv1D(60, 60, strides = 1)(Dropout3)\n",
        "    Batch1 = BatchNormalization()(Conv1D1)\n",
        "    Relu1 = Activation('relu')(Batch1)\n",
        "    Conv1D2 = Conv1D(30, 30, strides = 1)(Relu1)\n",
        "    Batch2 = BatchNormalization()(Conv1D2)\n",
        "    Relu2 = Activation('relu')(Batch2)\n",
        "    # Conv1D3 = Conv1D(15, 3, strides = 1)(Relu2)\n",
        "    # Batch3 = BatchNormalization()(Conv1D3)\n",
        "    # Relu3 = Activation('relu')(Batch3)\n",
        "\n",
        "    Softmax1 = Softmax(axis=-2, name='softmax1')(Relu2)\n",
        "    #Dense1 = Dense(24, activation='tanh')(Softmax1)\n",
        "    #Dense2 = Dense(24, activation='softmax', name='softmax')(Dense1)\n",
        "    Dense2 = Dense(24)(Softmax1)\n",
        "    Flatten1 = Flatten()(Dense2)\n",
        "    output = Dense(2, activation='softmax',name='mainoutput')(Flatten1)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4P9ViqDtK78d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import U\n",
        "def build_banet_model_middle():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SMEGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 30\n",
        "    # LSTM1 = LSTM(lstm_units, return_sequences=True)(singleinput) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    # Dropout1 = Dropout(0.5)(LSTM1)\n",
        "    # LSTM2 = LSTM(lstm_units, return_sequences=True)(Dropout1)\n",
        "    # Dropout2 = Dropout(0.5)(LSTM2)\n",
        "    # LSTM3 = LSTM(lstm_units, return_sequences=True)(Dropout2)\n",
        "    #Dropout3 = Dropout(0.5)(LSTM3)\n",
        "\n",
        "    Conv1D1 = Conv1D(60, 60, strides = 1)(singleinput)\n",
        "    Batch1 = BatchNormalization()(Conv1D1)\n",
        "    Relu1 = Activation('tanh')(Batch1)\n",
        "    Dropout1 = Dropout(0.5)(Relu1)\n",
        "\n",
        "    Conv1D2 = Conv1D(30, 30, strides = 1)(Dropout1)\n",
        "    Batch2 = BatchNormalization()(Conv1D2)\n",
        "    Relu2 = Activation('tanh')(Batch2)\n",
        "    Dropout2 = Dropout(0.5)(Relu2)\n",
        "\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout2])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    singleinput_SEMG = Input(shape=(180, 2,))\n",
        "    lstm_units = 30\n",
        "    # LSTM1_SEMG = LSTM(lstm_units, return_sequences=True)(singleinput_SEMG) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    # Dropout1_SEMG = Dropout(0.5)(LSTM1_SEMG)\n",
        "    # LSTM2_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout1_SEMG)\n",
        "    # Dropout2_SEMG = Dropout(0.5)(LSTM2_SEMG)\n",
        "    # LSTM3_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout2_SEMG)\n",
        "    # Dropout3_SEMG = Dropout(0.5)(LSTM3_SEMG)\n",
        "\n",
        "    Conv1D1_SEMG = Conv1D(60, 60, strides = 1)(singleinput_SEMG)\n",
        "    Batch1_SEMG = BatchNormalization()(Conv1D1_SEMG)\n",
        "    Relu1_SEMG = Activation('tanh')(Batch1_SEMG)\n",
        "    Dropout1_SEMG = Dropout(0.75)(Relu1_SEMG)\n",
        "\n",
        "    Conv1D2_SEMG = Conv1D(30, 30, strides = 1)(Dropout1_SEMG)\n",
        "    Batch2_SEMG = BatchNormalization()(Conv1D2_SEMG)\n",
        "    Relu2_SEMG = Activation('tanh')(Batch2_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.75)(Relu2_SEMG)\n",
        "\n",
        "    TemporalProcessmodel_SEMG = Model(inputs=[singleinput_SEMG], outputs=[Dropout2_SEMG])\n",
        "\n",
        "\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 70,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    lr = crop(2, 66, 67)(inputs)\n",
        "    ll = crop(2, 67, 68)(inputs)\n",
        "    SEMG_lower_back = concatenate([lr, ll], axis=-1)\n",
        "\n",
        "    SEMGout1 = TemporalProcessmodel_SEMG(SEMG_lower_back)\n",
        "\n",
        "    TemporalAttention_SEMG1 = Conv1D(1, 1, strides=1, padding = 'same')(SEMGout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG1 = Softmax(axis=-2, name='TemporalAttenSEMG1')(TemporalAttention_SEMG1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout1 = multiply([SEMGout1, TemporalAttention_SEMG1])\n",
        "    SEMGout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout1)\n",
        "    lower_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout1)\n",
        "\n",
        "\n",
        "    ur = crop(2, 68, 69)(inputs)\n",
        "    ul = crop(2, 69, 70)(inputs)\n",
        "    SEMG_upper_back = concatenate([ur, ul], axis=-1)\n",
        "\n",
        "    SEMGout2 = TemporalProcessmodel_SEMG(SEMG_upper_back)\n",
        "\n",
        "    TemporalAttention_SEMG2 = Conv1D(1, 1, strides=1, padding = 'same')(SEMGout2) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG2 = Softmax(axis=-2, name='TemporalAttenSEMG2')(TemporalAttention_SEMG2) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout2 = multiply([SEMGout2, TemporalAttention_SEMG2])\n",
        "    SEMGout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout2)\n",
        "    upper_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout2)\n",
        "\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44, 45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1, padding = 'same')(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22, lower_back, upper_back\n",
        "                        ], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(24, activation='tanh')(DATA)\n",
        "    a = Dense(24, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output = Dense(2, activation='softmax',name='mainoutput')(attentionresult)\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OqAnCILELK4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import U\n",
        "def build_banet_model_late():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SMEGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 30\n",
        "    '''\n",
        "    lstm_units = 32\n",
        "    LSTM1 = LSTM(lstm_units, return_sequences=True)(singleinput) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1 = Dropout(0.5)(LSTM1)\n",
        "    LSTM2 = LSTM(lstm_units, return_sequences=True)(Dropout1)\n",
        "    Dropout2 = Dropout(0.5)(LSTM2)\n",
        "    LSTM3 = LSTM(lstm_units, return_sequences=True)(Dropout2)\n",
        "    Dropout3 = Dropout(0.5)(LSTM3)\n",
        "    '''\n",
        "\n",
        "\n",
        "    Conv1D1 = Conv1D(60, 60, strides = 1)(singleinput)\n",
        "    Batch1 = BatchNormalization()(Conv1D1)\n",
        "    Relu1 = Activation('relu')(Batch1)\n",
        "    Dropout1 = Dropout(0.5)(Relu1)\n",
        "\n",
        "    Conv1D2 = Conv1D(30, 30, strides = 1)(Dropout1)\n",
        "    Batch2 = BatchNormalization()(Conv1D2)\n",
        "    Relu2 = Activation('relu')(Batch2)\n",
        "    Dropout2 = Dropout(0.25)(Relu2)\n",
        "\n",
        "    Conv1D3 = Conv1D(15, 15, strides = 1)(Dropout2)\n",
        "    Batch3 = BatchNormalization()(Conv1D3)\n",
        "    Relu3 = Activation('relu')(Batch3)\n",
        "    Dropout3 = Dropout(0.25)(Relu3)\n",
        "\n",
        "    # MaxPool1 = MaxPooling1D(pool_size = 1)(Dropout3)\n",
        "    # Dropout3 = Dropout(0.5)(MaxPool1)\n",
        "\n",
        "\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout3])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    singleinput_SEMG = Input(shape=(180, 2,))\n",
        "    '''\n",
        "    lstm_units = 32\n",
        "    LSTM1_SEMG = LSTM(lstm_units, return_sequences=True)(singleinput_SEMG) # Refer to https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM to decide if 'CuDNN' is needed.\n",
        "    Dropout1_SEMG = Dropout(0.5)(LSTM1_SEMG)\n",
        "    LSTM2_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout1_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.5)(LSTM2_SEMG)\n",
        "    LSTM3_SEMG = LSTM(lstm_units, return_sequences=True)(Dropout2_SEMG)\n",
        "    Dropout3_SEMG = Dropout(0.5)(LSTM3_SEMG)\n",
        "    '''\n",
        "\n",
        "    Conv1D1_SEMG = Conv1D(60, 60, strides = 1)(singleinput_SEMG)\n",
        "    Batch1_SEMG = BatchNormalization()(Conv1D1_SEMG)\n",
        "    Relu1_SEMG = Activation('relu')(Batch1_SEMG)\n",
        "    Dropout1_SEMG = Dropout(0.5)(Relu1_SEMG)\n",
        "\n",
        "    Conv1D2_SEMG = Conv1D(30, 30, strides = 1)(Dropout1_SEMG)\n",
        "    Batch2_SEMG = BatchNormalization()(Conv1D2_SEMG)\n",
        "    Relu2_SEMG = Activation('relu')(Batch2_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.25)(Relu2_SEMG)\n",
        "\n",
        "    Conv1D3_SEMG = Conv1D(15, 15, strides = 1)(Dropout2_SEMG)\n",
        "    Batch3_SEMG = BatchNormalization()(Conv1D3_SEMG)\n",
        "    Relu3_SEMG = Activation('relu')(Batch3_SEMG)\n",
        "    Dropout3_SEMG = Dropout(0.25)(Relu3_SEMG)\n",
        "\n",
        "    # MaxPool1_SEMG = MaxPooling1D(pool_size = 1)(Dropout3_SEMG)\n",
        "    # Dropout3_SEMG = Dropout(0.5)(MaxPool1_SEMG)\n",
        "\n",
        "    TemporalProcessmodel_SEMG = Model(inputs=[singleinput_SEMG], outputs=[Dropout3_SEMG])\n",
        "\n",
        "\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 70,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    lr = crop(2, 66, 67)(inputs)\n",
        "    ll = crop(2, 67, 68)(inputs)\n",
        "    SEMG_lower_back = concatenate([lr, ll], axis=-1)\n",
        "\n",
        "    SEMGout1 = TemporalProcessmodel_SEMG(SEMG_lower_back)\n",
        "\n",
        "    TemporalAttention_SEMG1 = Conv1D(1, 1, strides=1)(SEMGout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG1 = Softmax(axis=-2, name='TemporalAttenSEMG1')(TemporalAttention_SEMG1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout1 = multiply([SEMGout1, TemporalAttention_SEMG1])\n",
        "    SEMGout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout1)\n",
        "    lower_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout1)\n",
        "\n",
        "\n",
        "    ur = crop(2, 68, 69)(inputs)\n",
        "    ul = crop(2, 69, 70)(inputs)\n",
        "    SEMG_upper_back = concatenate([ur, ul], axis=-1)\n",
        "\n",
        "    SEMGout2 = TemporalProcessmodel_SEMG(SEMG_upper_back)\n",
        "\n",
        "    TemporalAttention_SEMG2 = Conv1D(1, 1, strides=1)(SEMGout2) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG2 = Softmax(axis=-2, name='TemporalAttenSEMG2')(TemporalAttention_SEMG2) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout2 = multiply([SEMGout2, TemporalAttention_SEMG2])\n",
        "    SEMGout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout2)\n",
        "    upper_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout2)\n",
        "\n",
        "\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44, 45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1)(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1)(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1)(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1)(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1)(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1)(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1)(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1)(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1)(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1)(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1)(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1)(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1)(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1)(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1)(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1)(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1)(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1)(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1)(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1)(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1)(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1)(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22\n",
        "                        ], axis=2)\n",
        "\n",
        "    DATA_SEMG = concatenate([lower_back, upper_back], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(BodyNum, activation='tanh')(DATA)\n",
        "    a = Dense(BodyNum, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output1 = Dense(2, activation='softmax',name='suboutput1')(attentionresult)\n",
        "\n",
        "    # SEMG Attention Module\n",
        "    a_SEMG = Dense(SMEGNum, activation='tanh')(DATA_SEMG)\n",
        "    a_SEMG = Dense(SMEGNum, activation='softmax', name='semgattention')(a_SEMG)\n",
        "    attentionresult_SEMG = multiply([DATA_SEMG, a_SEMG])\n",
        "    attentionresult_SEMG = Flatten()(attentionresult_SEMG)\n",
        "\n",
        "    output2 = Dense(2, activation='softmax',name='suboutput2')(attentionresult_SEMG)\n",
        "\n",
        "    # ensemble\n",
        "    output = (output1 + output2)/2\n",
        "    # output = 0.2*output1 + 0.8*output2\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vLtFr01NLNd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import U\n",
        "def build_banet_model_late_temp_best():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SMEGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 30\n",
        "\n",
        "\n",
        "    Conv1D1 = Conv1D(60, 60, strides = 1)(singleinput)\n",
        "    Batch1 = BatchNormalization()(Conv1D1)\n",
        "    Relu1 = Activation('relu')(Batch1)\n",
        "    Dropout1 = Dropout(0.5)(Relu1)\n",
        "\n",
        "    Conv1D2 = Conv1D(30, 30, strides = 1)(Dropout1)\n",
        "    Batch2 = BatchNormalization()(Conv1D2)\n",
        "    Relu2 = Activation('relu')(Batch2)\n",
        "    Dropout2 = Dropout(0.5)(Relu2)\n",
        "\n",
        "    # MaxPool1 = MaxPooling1D(pool_size = 1)(Dropout3)\n",
        "    # Dropout3 = Dropout(0.5)(MaxPool1)\n",
        "\n",
        "\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout2])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    singleinput_SEMG = Input(shape=(180, 2,))\n",
        "\n",
        "    Conv1D1_SEMG = Conv1D(60, 60, strides = 1)(singleinput_SEMG)\n",
        "    Batch1_SEMG = BatchNormalization()(Conv1D1_SEMG)\n",
        "    Relu1_SEMG = Activation('relu')(Batch1_SEMG)\n",
        "    Dropout1_SEMG = Dropout(0.5)(Relu1_SEMG)\n",
        "\n",
        "    Conv1D2_SEMG = Conv1D(30, 30, strides = 1)(Dropout1_SEMG)\n",
        "    Batch2_SEMG = BatchNormalization()(Conv1D2_SEMG)\n",
        "    Relu2_SEMG = Activation('relu')(Batch2_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.5)(Relu2_SEMG)\n",
        "\n",
        "    # MaxPool1_SEMG = MaxPooling1D(pool_size = 1)(Dropout3_SEMG)\n",
        "    # Dropout3_SEMG = Dropout(0.5)(MaxPool1_SEMG)\n",
        "\n",
        "    TemporalProcessmodel_SEMG = Model(inputs=[singleinput_SEMG], outputs=[Dropout2_SEMG])\n",
        "\n",
        "\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 70,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    lr = crop(2, 66, 67)(inputs)\n",
        "    ll = crop(2, 67, 68)(inputs)\n",
        "    SEMG_lower_back = concatenate([lr, ll], axis=-1)\n",
        "\n",
        "    SEMGout1 = TemporalProcessmodel_SEMG(SEMG_lower_back)\n",
        "\n",
        "    TemporalAttention_SEMG1 = Conv1D(1, 1, strides=1)(SEMGout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG1 = Softmax(axis=-2, name='TemporalAttenSEMG1')(TemporalAttention_SEMG1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout1 = multiply([SEMGout1, TemporalAttention_SEMG1])\n",
        "    SEMGout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout1)\n",
        "    lower_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout1)\n",
        "\n",
        "\n",
        "    ur = crop(2, 68, 69)(inputs)\n",
        "    ul = crop(2, 69, 70)(inputs)\n",
        "    SEMG_upper_back = concatenate([ur, ul], axis=-1)\n",
        "\n",
        "    SEMGout2 = TemporalProcessmodel_SEMG(SEMG_upper_back)\n",
        "\n",
        "    TemporalAttention_SEMG2 = Conv1D(1, 1, strides=1)(SEMGout2) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG2 = Softmax(axis=-2, name='TemporalAttenSEMG2')(TemporalAttention_SEMG2) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout2 = multiply([SEMGout2, TemporalAttention_SEMG2])\n",
        "    SEMGout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout2)\n",
        "    upper_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout2)\n",
        "\n",
        "\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44, 45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1)(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1)(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1)(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1)(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1)(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1)(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1)(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1)(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1)(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1)(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1)(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1)(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1)(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1)(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1)(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1)(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1)(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1)(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1)(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1)(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1)(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1)(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22\n",
        "                        ], axis=2)\n",
        "\n",
        "    DATA_SEMG = concatenate([lower_back, upper_back], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(BodyNum, activation='tanh')(DATA)\n",
        "    a = Dense(BodyNum, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output1 = Dense(2, activation='softmax',name='suboutput1')(attentionresult)\n",
        "\n",
        "    # SEMG Attention Module\n",
        "    a_SEMG = Dense(SMEGNum, activation='tanh')(DATA_SEMG)\n",
        "    a_SEMG = Dense(SMEGNum, activation='softmax', name='semgattention')(a_SEMG)\n",
        "    attentionresult_SEMG = multiply([DATA_SEMG, a_SEMG])\n",
        "    attentionresult_SEMG = Flatten()(attentionresult_SEMG)\n",
        "\n",
        "    output2 = Dense(2, activation='softmax',name='suboutput2')(attentionresult_SEMG)\n",
        "\n",
        "    # ensemble\n",
        "    output = (output1 + output2)/2\n",
        "    # output = 0.2*output1 + 0.8*output2\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ExGdQ2NE-XAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import U\n",
        "def build_banet_model_late_test():\n",
        "    timestep = 180   # length of an input frame\n",
        "    dimension = 66   # dimension of an input frame, 66 = 22 joints by 3 xyz coordinates, the 4 coordinates of the foot are removed.\n",
        "    BodyNum = 22     # number of body segments (different sensors) to consider\n",
        "    SMEGNum = 2\n",
        "\n",
        "    #Model 1: Temporal Information encoding model for BANet (keras Model API)\n",
        "    singleinput = Input(shape=(180, 3,))\n",
        "    lstm_units = 30\n",
        "\n",
        "\n",
        "    Conv1D1 = Conv1D(60, 60, strides = 1)(singleinput)\n",
        "    Batch1 = BatchNormalization()(Conv1D1)\n",
        "    Relu1 = Activation('relu')(Batch1)\n",
        "    Dropout1 = Dropout(0.5)(Relu1)\n",
        "\n",
        "    Conv1D2 = Conv1D(30, 30, strides = 1)(Dropout1)\n",
        "    Batch2 = BatchNormalization()(Conv1D2)\n",
        "    Relu2 = Activation('tanh')(Batch2)\n",
        "    Dropout2 = Dropout(0.5)(Relu2)\n",
        "\n",
        "    # MaxPool1 = MaxPooling1D(pool_size = 1)(Dropout3)\n",
        "    # Dropout3 = Dropout(0.5)(MaxPool1)\n",
        "\n",
        "\n",
        "    TemporalProcessmodel = Model(inputs=[singleinput], outputs=[Dropout2])\n",
        "    # TemporalProcessmodel.summary()\n",
        "\n",
        "    singleinput_SEMG = Input(shape=(180, 2,))\n",
        "\n",
        "    Conv1D1_SEMG = Conv1D(60, 60, strides = 1)(singleinput_SEMG)\n",
        "    Batch1_SEMG = BatchNormalization()(Conv1D1_SEMG)\n",
        "    Relu1_SEMG = Activation('relu')(Batch1_SEMG)\n",
        "    Dropout1_SEMG = Dropout(0.5)(Relu1_SEMG)\n",
        "\n",
        "    Conv1D2_SEMG = Conv1D(30, 30, strides = 1)(Dropout1_SEMG)\n",
        "    Batch2_SEMG = BatchNormalization()(Conv1D2_SEMG)\n",
        "    Relu2_SEMG = Activation('tanh')(Batch2_SEMG)\n",
        "    Dropout2_SEMG = Dropout(0.5)(Relu2_SEMG)\n",
        "\n",
        "    # MaxPool1_SEMG = MaxPooling1D(pool_size = 1)(Dropout3_SEMG)\n",
        "    # Dropout3_SEMG = Dropout(0.5)(MaxPool1_SEMG)\n",
        "\n",
        "    TemporalProcessmodel_SEMG = Model(inputs=[singleinput_SEMG], outputs=[Dropout2_SEMG])\n",
        "\n",
        "\n",
        "\n",
        "    # Model 2: Main Structure, starting with independent temporal information encoding and attention learning\n",
        "    inputs = Input(shape=(180, 70,))      # The input data is 180 timesteps by 66 features (22 joints by 3 xyz coordinates)\n",
        "    # The information each body segment provides is the coordinates of each joint\n",
        "\n",
        "    lr = crop(2, 66, 67)(inputs)\n",
        "    ll = crop(2, 67, 68)(inputs)\n",
        "    SEMG_lower_back = concatenate([lr, ll], axis=-1)\n",
        "\n",
        "    SEMGout1 = TemporalProcessmodel_SEMG(SEMG_lower_back)\n",
        "\n",
        "    TemporalAttention_SEMG1 = Conv1D(1, 1, strides=1)(SEMGout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG1 = Softmax(axis=-2, name='TemporalAttenSEMG1')(TemporalAttention_SEMG1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout1 = multiply([SEMGout1, TemporalAttention_SEMG1])\n",
        "    SEMGout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout1)\n",
        "    lower_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout1)\n",
        "\n",
        "\n",
        "    ur = crop(2, 68, 69)(inputs)\n",
        "    ul = crop(2, 69, 70)(inputs)\n",
        "    SEMG_upper_back = concatenate([ur, ul], axis=-1)\n",
        "\n",
        "    SEMGout2 = TemporalProcessmodel_SEMG(SEMG_upper_back)\n",
        "\n",
        "    TemporalAttention_SEMG2 = Conv1D(1, 1, strides=1)(SEMGout2) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention_SEMG2 = Softmax(axis=-2, name='TemporalAttenSEMG2')(TemporalAttention_SEMG2) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    SEMGout2 = multiply([SEMGout2, TemporalAttention_SEMG2])\n",
        "    SEMGout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(SEMGout2)\n",
        "    upper_back = Permute((2, 1), input_shape=(1, lstm_units))(SEMGout2)\n",
        "\n",
        "\n",
        "\n",
        "    x1 = crop(2, 0, 1)(inputs)\n",
        "    y1 = crop(2, 22, 23)(inputs)\n",
        "    z1 = crop(2, 44, 45)(inputs)\n",
        "    B1 = concatenate([x1, y1, z1], axis=-1)\n",
        "\n",
        "    Anglefullout1 = TemporalProcessmodel(B1)\n",
        "\n",
        "    TemporalAttention1 = Conv1D(1, 1, strides=1)(Anglefullout1) # Temporal Attention Module for each body segment will starts with 1 X 1 Convolution\n",
        "    TemporalAttention1 = Softmax(axis=-2, name='TemporalAtten1')(TemporalAttention1) # You need Keras >= 2.1.3 to call Softmax as a layer\n",
        "    AngleAttout1 = multiply([Anglefullout1, TemporalAttention1])\n",
        "    AngleAttout1 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout1)\n",
        "    Blast1 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout1)\n",
        "\n",
        "    x2 = crop(2, 1, 2)(inputs)\n",
        "    y2 = crop(2, 23, 24)(inputs)\n",
        "    z2 = crop(2, 45, 46)(inputs)\n",
        "    B2 = concatenate([x2, y2, z2], axis=-1)\n",
        "    Anglefullout2 = TemporalProcessmodel(B2)\n",
        "\n",
        "    TemporalAttention2 = Conv1D(1, 1, strides=1)(Anglefullout2)\n",
        "    TemporalAttention2 = Softmax(axis=-2, name='TemporalAtten2')(TemporalAttention2)\n",
        "    AngleAttout2 = multiply([Anglefullout2, TemporalAttention2])\n",
        "    AngleAttout2 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout2)\n",
        "    Blast2 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout2)\n",
        "\n",
        "    x3 = crop(2, 2, 3)(inputs)\n",
        "    y3 = crop(2, 24, 25)(inputs)\n",
        "    z3 = crop(2, 46, 47)(inputs)\n",
        "    B3 = concatenate([x3, y3, z3], axis=-1)\n",
        "    Anglefullout3 = TemporalProcessmodel(B3)\n",
        "    TemporalAttention3 = Conv1D(1, 1, strides=1)(Anglefullout3)\n",
        "    TemporalAttention3 = Softmax(axis=-2, name='TemporalAtten3')(TemporalAttention3)\n",
        "    AngleAttout3 = multiply([Anglefullout3, TemporalAttention3])\n",
        "    AngleAttout3 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout3)\n",
        "    Blast3 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout3)\n",
        "\n",
        "    x4 = crop(2, 3, 4)(inputs)\n",
        "    y4 = crop(2, 25, 26)(inputs)\n",
        "    z4 = crop(2, 47, 48)(inputs)\n",
        "    B4 = concatenate([x4, y4, z4], axis=-1)\n",
        "    Anglefullout4 = TemporalProcessmodel(B4)\n",
        "    TemporalAttention4 = Conv1D(1, 1, strides=1)(Anglefullout4)\n",
        "    TemporalAttention4 = Softmax(axis=-2, name='TemporalAtten4')(TemporalAttention4)\n",
        "    AngleAttout4 = multiply([Anglefullout4, TemporalAttention4])\n",
        "    AngleAttout4 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout4)\n",
        "    Blast4 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout4)\n",
        "\n",
        "    x5 = crop(2, 4, 5)(inputs)\n",
        "    y5 = crop(2, 26, 27)(inputs)\n",
        "    z5 = crop(2, 48, 49)(inputs)\n",
        "    B5 = concatenate([x5, y5, z5], axis=-1)\n",
        "    Anglefullout5 = TemporalProcessmodel(B5)\n",
        "    TemporalAttention5 = Conv1D(1, 1, strides=1)(Anglefullout5)\n",
        "    TemporalAttention5 = Softmax(axis=-2, name='TemporalAtten5')(TemporalAttention5)\n",
        "    AngleAttout5 = multiply([Anglefullout5, TemporalAttention5])\n",
        "    AngleAttout5 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout5)\n",
        "    Blast5 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout5)\n",
        "\n",
        "    x6 = crop(2, 5, 6)(inputs)\n",
        "    y6 = crop(2, 27, 28)(inputs)\n",
        "    z6 = crop(2, 49, 50)(inputs)\n",
        "    B6 = concatenate([x6, y6, z6], axis=-1)\n",
        "    Anglefullout6 = TemporalProcessmodel(B6)\n",
        "    TemporalAttention6 = Conv1D(1, 1, strides=1)(Anglefullout6)\n",
        "    TemporalAttention6 = Softmax(axis=-2, name='TemporalAtten6')(TemporalAttention6)\n",
        "    AngleAttout6 = multiply([Anglefullout6, TemporalAttention6])\n",
        "    AngleAttout6 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout6)\n",
        "    Blast6 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout6)\n",
        "\n",
        "    x7 = crop(2, 6, 7)(inputs)\n",
        "    y7 = crop(2, 28, 29)(inputs)\n",
        "    z7 = crop(2, 50, 51)(inputs)\n",
        "    B7 = concatenate([x7, y7, z7], axis=-1)\n",
        "    Anglefullout7 = TemporalProcessmodel(B7)\n",
        "    TemporalAttention7 = Conv1D(1, 1, strides=1)(Anglefullout7)\n",
        "    TemporalAttention7 = Softmax(axis=-2, name='TemporalAtten7')(TemporalAttention7)\n",
        "    AngleAttout7 = multiply([Anglefullout7, TemporalAttention7])\n",
        "    AngleAttout7 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout7)\n",
        "    Blast7 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout7)\n",
        "\n",
        "    x8 = crop(2, 7, 8)(inputs)\n",
        "    y8 = crop(2, 29, 30)(inputs)\n",
        "    z8 = crop(2, 51, 52)(inputs)\n",
        "    B8 = concatenate([x8, y8, z8], axis=-1)\n",
        "    Anglefullout8 = TemporalProcessmodel(B8)\n",
        "    TemporalAttention8 = Conv1D(1, 1, strides=1)(Anglefullout8)\n",
        "    TemporalAttention8 = Softmax(axis=-2, name='TemporalAtten8')(TemporalAttention8)\n",
        "    AngleAttout8 = multiply([Anglefullout8, TemporalAttention8])\n",
        "    AngleAttout8 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout8)\n",
        "    Blast8 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout8)\n",
        "\n",
        "    x9 = crop(2, 8, 9)(inputs)\n",
        "    y9 = crop(2, 30, 31)(inputs)\n",
        "    z9 = crop(2, 52, 53)(inputs)\n",
        "    B9 = concatenate([x9, y9, z9], axis=-1)\n",
        "    Anglefullout9 = TemporalProcessmodel(B9)\n",
        "    TemporalAttention9 = Conv1D(1, 1, strides=1)(Anglefullout9)\n",
        "    TemporalAttention9 = Softmax(axis=-2, name='TemporalAtten9')(TemporalAttention9)\n",
        "    AngleAttout9 = multiply([Anglefullout9, TemporalAttention9])\n",
        "    AngleAttout9 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout9)\n",
        "    Blast9 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout9)\n",
        "\n",
        "    x10 = crop(2, 9, 10)(inputs)\n",
        "    y10 = crop(2, 31, 32)(inputs)\n",
        "    z10 = crop(2, 53, 54)(inputs)\n",
        "    B10 = concatenate([x10, y10, z10], axis=-1)\n",
        "    Anglefullout10 = TemporalProcessmodel(B10)\n",
        "    TemporalAttention10 = Conv1D(1, 1, strides=1)(Anglefullout10)\n",
        "    TemporalAttention10 = Softmax(axis=-2, name='TemporalAtten10')(TemporalAttention10)\n",
        "    AngleAttout10 = multiply([Anglefullout10, TemporalAttention10])\n",
        "    AngleAttout10 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout10)\n",
        "    Blast10 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout10)\n",
        "\n",
        "    x11 = crop(2, 10, 11)(inputs)\n",
        "    y11 = crop(2, 32, 33)(inputs)\n",
        "    z11 = crop(2, 54, 55)(inputs)\n",
        "    B11 = concatenate([x11, y11, z11], axis=-1)\n",
        "    Anglefullout11 = TemporalProcessmodel(B11)\n",
        "    TemporalAttention11 = Conv1D(1, 1, strides=1)(Anglefullout11)\n",
        "    TemporalAttention11 = Softmax(axis=-2, name='TemporalAtten11')(TemporalAttention11)\n",
        "    AngleAttout11 = multiply([Anglefullout11, TemporalAttention11])\n",
        "    AngleAttout11 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout11)\n",
        "    Blast11 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout11)\n",
        "\n",
        "    x12 = crop(2, 11, 12)(inputs)\n",
        "    y12 = crop(2, 33, 34)(inputs)\n",
        "    z12 = crop(2, 55, 56)(inputs)\n",
        "    B12 = concatenate([x12, y12, z12], axis=-1)\n",
        "    Anglefullout12 = TemporalProcessmodel(B12)\n",
        "    TemporalAttention12 = Conv1D(1, 1, strides=1)(Anglefullout12)\n",
        "    TemporalAttention12 = Softmax(axis=-2, name='TemporalAtten12')(TemporalAttention12)\n",
        "    AngleAttout12 = multiply([Anglefullout12, TemporalAttention12])\n",
        "    AngleAttout12 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout12)\n",
        "    Blast12 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout12)\n",
        "\n",
        "    x13 = crop(2, 12, 13)(inputs)\n",
        "    y13 = crop(2, 34, 35)(inputs)\n",
        "    z13 = crop(2, 56, 57)(inputs)\n",
        "    B13 = concatenate([x13, y13, z13], axis=-1)\n",
        "    Anglefullout13 = TemporalProcessmodel(B13)\n",
        "    TemporalAttention13 = Conv1D(1, 1, strides=1)(Anglefullout13)\n",
        "    TemporalAttention13 = Softmax(axis=-2, name='TemporalAtten13')(TemporalAttention13)\n",
        "    AngleAttout13 = multiply([Anglefullout13, TemporalAttention13])\n",
        "    AngleAttout13 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout13)\n",
        "    Blast13 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout13)\n",
        "\n",
        "    x14 = crop(2, 13, 14)(inputs)\n",
        "    y14 = crop(2, 35, 36)(inputs)\n",
        "    z14 = crop(2, 57, 58)(inputs)\n",
        "    B14 = concatenate([x14, y14, z14], axis=-1)\n",
        "    Anglefullout14 = TemporalProcessmodel(B14)\n",
        "    TemporalAttention14 = Conv1D(1, 1, strides=1)(Anglefullout14)\n",
        "    TemporalAttention14 = Softmax(axis=-2, name='TemporalAtten14')(TemporalAttention14)\n",
        "    AngleAttout14 = multiply([Anglefullout14, TemporalAttention14])\n",
        "    AngleAttout14 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout14)\n",
        "    Blast14 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout14)\n",
        "\n",
        "    x15 = crop(2, 14, 15)(inputs)\n",
        "    y15 = crop(2, 36, 37)(inputs)\n",
        "    z15 = crop(2, 58, 59)(inputs)\n",
        "    B15 = concatenate([x15, y15, z15], axis=-1)\n",
        "    Anglefullout15 = TemporalProcessmodel(B15)\n",
        "    TemporalAttention15 = Conv1D(1, 1, strides=1)(Anglefullout15)\n",
        "    TemporalAttention15 = Softmax(axis=-2, name='TemporalAtten15')(TemporalAttention15)\n",
        "    AngleAttout15 = multiply([Anglefullout15, TemporalAttention15])\n",
        "    AngleAttout15 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout15)\n",
        "    Blast15 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout15)\n",
        "\n",
        "    x16 = crop(2, 15, 16)(inputs)\n",
        "    y16 = crop(2, 37, 38)(inputs)\n",
        "    z16 = crop(2, 59, 60)(inputs)\n",
        "    B16 = concatenate([x16, y16, z16], axis=-1)\n",
        "    Anglefullout16 = TemporalProcessmodel(B16)\n",
        "    TemporalAttention16 = Conv1D(1, 1, strides=1)(Anglefullout16)\n",
        "    TemporalAttention16 = Softmax(axis=-2, name='TemporalAtten16')(TemporalAttention16)\n",
        "    AngleAttout16 = multiply([Anglefullout16, TemporalAttention16])\n",
        "    AngleAttout16 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout16)\n",
        "    Blast16 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout16)\n",
        "\n",
        "    x17 = crop(2, 16, 17)(inputs)\n",
        "    y17 = crop(2, 38, 39)(inputs)\n",
        "    z17 = crop(2, 60, 61)(inputs)\n",
        "    B17 = concatenate([x17, y17, z17], axis=-1)\n",
        "    Anglefullout17 = TemporalProcessmodel(B17)\n",
        "    TemporalAttention17 = Conv1D(1, 1, strides=1)(Anglefullout17)\n",
        "    TemporalAttention17 = Softmax(axis=-2, name='TemporalAtten17')(TemporalAttention17)\n",
        "    AngleAttout17 = multiply([Anglefullout17, TemporalAttention17])\n",
        "    AngleAttout17 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout17)\n",
        "    Blast17 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout17)\n",
        "\n",
        "    x18 = crop(2, 17, 18)(inputs)\n",
        "    y18 = crop(2, 39, 40)(inputs)\n",
        "    z18 = crop(2, 61, 62)(inputs)\n",
        "    B18 = concatenate([x18, y18, z18], axis=-1)\n",
        "    Anglefullout18 = TemporalProcessmodel(B18)\n",
        "    TemporalAttention18 = Conv1D(1, 1, strides=1)(Anglefullout18)\n",
        "    TemporalAttention18 = Softmax(axis=-2, name='TemporalAtten18')(TemporalAttention18)\n",
        "    AngleAttout18 = multiply([Anglefullout18, TemporalAttention18])\n",
        "    AngleAttout18 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout18)\n",
        "    Blast18 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout18)\n",
        "\n",
        "    x19 = crop(2, 18, 19)(inputs)\n",
        "    y19 = crop(2, 40, 41)(inputs)\n",
        "    z19 = crop(2, 62, 63)(inputs)\n",
        "    B19 = concatenate([x19, y19, z19], axis=-1)\n",
        "    Anglefullout19 = TemporalProcessmodel(B19)\n",
        "    TemporalAttention19 = Conv1D(1, 1, strides=1)(Anglefullout19)\n",
        "    TemporalAttention19 = Softmax(axis=-2, name='TemporalAtten19')(TemporalAttention19)\n",
        "    AngleAttout19 = multiply([Anglefullout19, TemporalAttention19])\n",
        "    AngleAttout19 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout19)\n",
        "    Blast19 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout19)\n",
        "\n",
        "    x20 = crop(2, 19, 20)(inputs)\n",
        "    y20 = crop(2, 41, 42)(inputs)\n",
        "    z20 = crop(2, 63, 64)(inputs)\n",
        "    B20 = concatenate([x20, y20, z20], axis=-1)\n",
        "    Anglefullout20 = TemporalProcessmodel(B20)\n",
        "    TemporalAttention20 = Conv1D(1, 1, strides=1)(Anglefullout20)\n",
        "    TemporalAttention20 = Softmax(axis=-2, name='TemporalAtten20')(TemporalAttention20)\n",
        "    AngleAttout20 = multiply([Anglefullout20, TemporalAttention20])\n",
        "    AngleAttout20 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout20)\n",
        "    Blast20 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout20)\n",
        "\n",
        "    x21 = crop(2, 20, 21)(inputs)\n",
        "    y21 = crop(2, 42, 43)(inputs)\n",
        "    z21 = crop(2, 64, 65)(inputs)\n",
        "    B21 = concatenate([x21, y21, z21], axis=-1)\n",
        "    Anglefullout21 = TemporalProcessmodel(B21)\n",
        "    TemporalAttention21 = Conv1D(1, 1, strides=1)(Anglefullout21)\n",
        "    TemporalAttention21 = Softmax(axis=-2, name='TemporalAtten21')(TemporalAttention21)\n",
        "    AngleAttout21 = multiply([Anglefullout21, TemporalAttention21])\n",
        "    AngleAttout21 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout21)\n",
        "    Blast21 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout21)\n",
        "\n",
        "    x22 = crop(2, 21, 22)(inputs)\n",
        "    y22 = crop(2, 43, 44)(inputs)\n",
        "    z22 = crop(2, 65, 66)(inputs)\n",
        "    B22 = concatenate([x22, y22, z22], axis=-1)\n",
        "    Anglefullout22 = TemporalProcessmodel(B22)\n",
        "    TemporalAttention22 = Conv1D(1, 1, strides=1)(Anglefullout22)\n",
        "    TemporalAttention22 = Softmax(axis=-2, name='TemporalAtten22')(TemporalAttention22)\n",
        "    AngleAttout22 = multiply([Anglefullout22, TemporalAttention22])\n",
        "    AngleAttout22 = Lambda(lambda x: sum(x, axis=1, keepdims=True))(AngleAttout22)\n",
        "    Blast22 = Permute((2, 1), input_shape=(1, lstm_units))(AngleAttout22)\n",
        "\n",
        "    # Model 3: Feature Concatenation for Bodily Attention Learning\n",
        "    # The size of the output from each body segment is k X 1, while k is the number of LSTM hidden units\n",
        "    # In early experiments, we found that it is better to keep the dimension k instead of merging them into one\n",
        "\n",
        "    DATA = concatenate([Blast1, Blast2, Blast3, Blast4, Blast5, Blast6, Blast7, Blast8,\n",
        "                        Blast9, Blast10, Blast11, Blast12, Blast13, Blast14, Blast15, Blast16,\n",
        "                        Blast17, Blast18, Blast19, Blast20, Blast21, Blast22\n",
        "                        ], axis=2)\n",
        "\n",
        "    DATA_SEMG = concatenate([lower_back, upper_back], axis=2)\n",
        "\n",
        "    # Bodily Attention Module\n",
        "    a = Dense(BodyNum, activation='tanh')(DATA)\n",
        "    a = Dense(BodyNum, activation='softmax', name='bodyattention')(a)\n",
        "    attentionresult = multiply([DATA, a])\n",
        "    attentionresult = Flatten()(attentionresult)\n",
        "\n",
        "    output1 = Dense(2, activation='softmax',name='suboutput1')(attentionresult)\n",
        "\n",
        "    # SEMG Attention Module\n",
        "    a_SEMG = Dense(SMEGNum, activation='tanh')(DATA_SEMG)\n",
        "    a_SEMG = Dense(SMEGNum, activation='softmax', name='semgattention')(a_SEMG)\n",
        "    attentionresult_SEMG = multiply([DATA_SEMG, a_SEMG])\n",
        "    attentionresult_SEMG = Flatten()(attentionresult_SEMG)\n",
        "\n",
        "    output2 = Dense(2, activation='softmax',name='suboutput2')(attentionresult_SEMG)\n",
        "\n",
        "    # ensemble\n",
        "    output = (output1 + output2)/2\n",
        "    # output = 0.2*output1 + 0.8*output2\n",
        "\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "    # model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uNzXagbQsusf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def jitter(X, y, std_dev):\n",
        "    noise = np.random.normal(0, std_dev, X.shape)\n",
        "    return X + noise, y\n",
        "\n",
        "def crop_new(X, y, probability):\n",
        "    mask = np.random.rand(*X.shape) < probability\n",
        "    return X * (1 - mask), y\n",
        "\n",
        "def augment_data(X, y):\n",
        "    X_jittered_1, y_jittered_1 = jitter(X, y, 0.05)\n",
        "    X_jittered_2, y_jittered_2 = jitter(X, y, 0.1)\n",
        "\n",
        "    X_cropped_1, y_cropped_1 = crop_new(X, y, 0.05)\n",
        "    X_cropped_2, y_cropped_2 = crop_new(X, y, 0.1)\n",
        "\n",
        "    X_augmented = np.concatenate((X, X_jittered_1, X_jittered_2, X_cropped_1, X_cropped_2), axis=0)\n",
        "    y_augmented = np.concatenate((y, y_jittered_1, y_jittered_2, y_cropped_1, y_cropped_2), axis=0)\n",
        "\n",
        "    X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=0)\n",
        "\n",
        "    return X_augmented, y_augmented"
      ],
      "metadata": {
        "id": "lAkdfQDxKZhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def new_segment_data(data: np.ndarray, window_size: int = 180, overlap_ratio: float = 0.75, by_type: bool = True, min_frame: int = 12) -> np.ndarray:\n",
        "    assert data.shape[0] > 0\n",
        "    assert window_size > 0\n",
        "    assert 0 <= overlap_ratio < 1\n",
        "    assert 0 <= min_frame < window_size\n",
        "\n",
        "    dim = data.shape[1]\n",
        "    instances = []\n",
        "\n",
        "    if not by_type:\n",
        "        instances.append(data)\n",
        "    else:\n",
        "        assert data.shape[1] >= 71\n",
        "\n",
        "        num_data = data.shape[0]\n",
        "        left, right = 0, 1\n",
        "        pre_type = -1\n",
        "        cur_type = data[left, 70]\n",
        "        while right < num_data:\n",
        "            if data[right, 70] == cur_type:\n",
        "                right += 1\n",
        "                continue\n",
        "\n",
        "            if right - left <= min_frame:\n",
        "                left = right\n",
        "                cur_type = data[left, 70]\n",
        "                right += 1\n",
        "                continue\n",
        "\n",
        "            new_instance = np.take(data, range(left, right), axis=0)\n",
        "            if pre_type == new_instance[0, 70]:\n",
        "                instances[-1] = np.vstack([instances[-1], new_instance])\n",
        "            else:\n",
        "                instances.append(new_instance)\n",
        "\n",
        "            left = right\n",
        "            pre_type = cur_type\n",
        "            cur_type = data[left, 70]\n",
        "            right += 1\n",
        "\n",
        "        new_instance = np.take(data, range(left, right), axis=0)\n",
        "        last = instances[-1]\n",
        "        if last[0, 70] == new_instance[0, 70]:\n",
        "            instances[-1] = np.vstack([last, new_instance])\n",
        "        else:\n",
        "            instances.append(new_instance)\n",
        "\n",
        "    step_size = int(window_size * (1 - overlap_ratio))\n",
        "    windows = []\n",
        "\n",
        "    for instance in instances:\n",
        "        if instance.shape[0] < window_size:\n",
        "            instance = np.vstack([instance, np.zeros((window_size - instance.shape[0], dim))])\n",
        "            windows.append(instance)\n",
        "            continue\n",
        "\n",
        "        if (instance.shape[0] - window_size) % step_size != 0:\n",
        "            pad_size = step_size - (instance.shape[0] - window_size) % step_size\n",
        "            instance = np.vstack([instance, np.zeros((pad_size, dim))])\n",
        "\n",
        "        for i in range(0, instance.shape[0] - window_size + 1, step_size):\n",
        "            windows.append(np.take(instance, range(i, i + window_size), axis=0))\n",
        "\n",
        "    return np.array(windows)"
      ],
      "metadata": {
        "id": "jX0kCapsKZhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_data(filenames, data_set,downsampling = False):\n",
        "    data_path = '/content/gdrive/MyDrive/Colab Notebooks/comp0053_dataset/'\n",
        "    X_train_list = []\n",
        "    y_train_list = []\n",
        "\n",
        "    selected_data_list = []\n",
        "    for file_name in filenames:\n",
        "        traindata = loadmat(data_path + data_set + '/' + file_name + '.mat')\n",
        "        count_table = np.unique(traindata['data'][:,72:73],return_counts=True)\n",
        "        if downsampling == True:\n",
        "            if (len(count_table[0])) == 2:\n",
        "                selected_data_list.append(traindata['data'])\n",
        "        else:\n",
        "            selected_data_list.append(traindata['data'])\n",
        "\n",
        "\n",
        "    for i in range(len(selected_data_list)):\n",
        "        processed_data = new_segment_data(selected_data_list[i], window_size=180, overlap_ratio=0.75, by_type=True, min_frame=12)\n",
        "\n",
        "        X_segmented = processed_data[:,:,0:70]\n",
        "        y_segmented = processed_data[:,:,72:73]\n",
        "\n",
        "        y_segmented = np.apply_along_axis(lambda x: mode(x)[0], 1, y_segmented[:, :, 0])\n",
        "\n",
        "        X_train_list.append(X_segmented)\n",
        "        y_train_list.append(y_segmented)\n",
        "\n",
        "    X_train_concat = np.concatenate(X_train_list, axis=0)\n",
        "    y_train_concat = np.concatenate(y_train_list, axis=0)\n",
        "\n",
        "    num_classes = 2  # classes\n",
        "\n",
        "    y_train_concat = to_categorical(y_train_concat, num_classes)\n",
        "    return X_train_concat, y_train_concat"
      ],
      "metadata": {
        "id": "p5c6mJaDKZhX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_participant_num = [\"C93D\",\"C382D\",\"C382N\",\"C544D\",\"C709N\",\"C788N\",\"P113D\",\"P113N\",\"P191D\",\"P191N\",\"P299D\",\"P300D\",\"P336D\",\"P492D\",\"P492N\",\"P531N\",\"P699D\",\"P890N\",\"P921D\",\"P921N\"]\n",
        "valid_participant_num = [\"C67D\",\"C202D\",\"C202N\",\"C256D\",\"C256N\",\"P54D\",\"P54N\",\"P342D\",\"P342N\",\"P487D\",\"P487N\",\"P649N\"]"
      ],
      "metadata": {
        "id": "Hvob2TAVKZhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_participant_num = [\"C56D\",\"C93D\",\"C382D\",\"C382N\",\"C544D\",\"C709N\",\"C788N\",\"P113D\",\"P113N\",\"P191D\",\"P191N\",\"P299D\",\"P299N\",\"P300D\",\"P336D\",\"P492D\",\"P492N\",\"P492D\",\"P531N\",\"P699D\",\"P699N\",\"P890N\",\"P921D\",\"P921N\"]\n",
        "valid_participant_num = [\"C67D\",\"C202D\",\"C202N\",\"C256D\",\"C256N\",\"P54D\",\"P54N\",\"P342D\",\"P342N\",\"P487D\",\"P487N\",\"P649N\"]"
      ],
      "metadata": {
        "id": "jJDrzMnhOROy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_participant_num = [\"C56D\",\"C93D\",\"C382D\",\"C382N\",\"C544D\",\"C709N\",\"C788N\",\"P113D\",\"P113N\",\"P191D\",\"P191N\",\"P299D\",\"P299N\",\"P300D\",\"P336D\",\"P492D\",\"P492N\",\"P531N\",\"P699D\",\"P699N\",\"P890N\",\"P921D\",\"P921N\"]\n",
        "valid_participant_num = [\"C67D\",\"C202D\",\"C202N\",\"C256D\",\"C256N\",\"P54D\",\"P54N\",\"P342D\",\"P342N\",\"P487D\",\"P487N\",\"P649N\"]"
      ],
      "metadata": {
        "id": "xtWNMY2OPXV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_train,y_train = load_data(train_participant_num,'train',downsampling=True)\n",
        "X_valid,y_valid = load_data(valid_participant_num,'validation')"
      ],
      "metadata": {
        "id": "NxvYgxeGKZhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5012, 180, 70),\n",
              " (array([0., 1.], dtype=float32), array([1053, 3959])),\n",
              " (2869, 180, 70),\n",
              " (array([0., 1.], dtype=float32), array([ 171, 2698])))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "X_train.shape,np.unique(y_train[:,0],return_counts=True),X_valid.shape,np.unique(y_valid[:,0],return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0gH7phbKZhb",
        "outputId": "fd57f18b-b123-4cb4-c320-3eede67f4b54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#X_train,y_train = augment_data(X_train,y_train)"
      ],
      "metadata": {
        "id": "5VClYsEFKZhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5012, 180, 70),\n",
              " (array([0., 1.], dtype=float32), array([1053, 3959])),\n",
              " (2869, 180, 70),\n",
              " (array([0., 1.], dtype=float32), array([ 171, 2698])))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "X_train.shape,np.unique(y_train[:,0],return_counts=True),X_valid.shape,np.unique(y_valid[:,0],return_counts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgNaAEx1KZhc",
        "outputId": "28866b92-bf96-4d07-cf23-920613d82eac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from keras import metrics,losses\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['binary_accuracy'])\n",
        "    plt.plot(history.history['val_binary_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0,1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def schedule(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * 0.95\n",
        "\n",
        "def model_pipeline(model):\n",
        "    initial_learning_rate = 0.0005\n",
        "    optimizer = Adam(lr=initial_learning_rate)\n",
        "\n",
        "    model.compile(loss=losses.BinaryFocalCrossentropy(), optimizer=optimizer, metrics=[metrics.binary_accuracy])\n",
        "\n",
        "    # Initialize the EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Initialize the LearningRateScheduler callback\n",
        "    lr_scheduler = LearningRateScheduler(schedule, verbose=1)\n",
        "\n",
        "    # Train the model with both callbacks\n",
        "    H = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=500,\n",
        "        epochs=30,\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        shuffle=False,\n",
        "        callbacks=[lr_scheduler],\n",
        "    )\n",
        "\n",
        "    # model.save_weights('model' + '.hdf5')\n",
        "    # model.load_weights('model' + '.hdf5')\n",
        "    y_predraw = model.predict(X_valid)\n",
        "    y_pred = np.argmax(y_predraw, axis=1)\n",
        "    y_true = np.argmax(y_valid, axis=1)\n",
        "\n",
        "    # Compute and display the additional evaluation metrics\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    return y_pred, y_true, H, model\n"
      ],
      "metadata": {
        "id": "-SslIbWzKZhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 35s 528ms/step - loss: 0.1517 - binary_accuracy: 0.7135 - val_loss: 0.1559 - val_binary_accuracy: 0.7564 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1263 - binary_accuracy: 0.7787 - val_loss: 0.0967 - val_binary_accuracy: 0.9317 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1189 - binary_accuracy: 0.8097 - val_loss: 0.0715 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1163 - binary_accuracy: 0.8142 - val_loss: 0.0617 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1138 - binary_accuracy: 0.8200 - val_loss: 0.0581 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1112 - binary_accuracy: 0.8222 - val_loss: 0.0568 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1106 - binary_accuracy: 0.8220 - val_loss: 0.0548 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1081 - binary_accuracy: 0.8232 - val_loss: 0.0544 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1083 - binary_accuracy: 0.8216 - val_loss: 0.0554 - val_binary_accuracy: 0.9400 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1051 - binary_accuracy: 0.8276 - val_loss: 0.0534 - val_binary_accuracy: 0.9407 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.1030 - binary_accuracy: 0.8276 - val_loss: 0.0558 - val_binary_accuracy: 0.9394 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1016 - binary_accuracy: 0.8330 - val_loss: 0.0548 - val_binary_accuracy: 0.9407 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0999 - binary_accuracy: 0.8350 - val_loss: 0.0559 - val_binary_accuracy: 0.9425 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0981 - binary_accuracy: 0.8338 - val_loss: 0.0566 - val_binary_accuracy: 0.9411 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0964 - binary_accuracy: 0.8370 - val_loss: 0.0589 - val_binary_accuracy: 0.9418 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0946 - binary_accuracy: 0.8398 - val_loss: 0.0604 - val_binary_accuracy: 0.9369 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0925 - binary_accuracy: 0.8410 - val_loss: 0.0611 - val_binary_accuracy: 0.9310 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0902 - binary_accuracy: 0.8430 - val_loss: 0.0617 - val_binary_accuracy: 0.9292 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0888 - binary_accuracy: 0.8466 - val_loss: 0.0671 - val_binary_accuracy: 0.9174 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0867 - binary_accuracy: 0.8448 - val_loss: 0.0650 - val_binary_accuracy: 0.9163 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0849 - binary_accuracy: 0.8540 - val_loss: 0.0693 - val_binary_accuracy: 0.9055 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0836 - binary_accuracy: 0.8542 - val_loss: 0.0660 - val_binary_accuracy: 0.9104 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0819 - binary_accuracy: 0.8579 - val_loss: 0.0705 - val_binary_accuracy: 0.8982 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0819 - binary_accuracy: 0.8549 - val_loss: 0.0729 - val_binary_accuracy: 0.8954 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0798 - binary_accuracy: 0.8623 - val_loss: 0.0695 - val_binary_accuracy: 0.8916 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0792 - binary_accuracy: 0.8633 - val_loss: 0.0746 - val_binary_accuracy: 0.8686 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0779 - binary_accuracy: 0.8685 - val_loss: 0.0707 - val_binary_accuracy: 0.8913 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0762 - binary_accuracy: 0.8701 - val_loss: 0.0808 - val_binary_accuracy: 0.8498 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0753 - binary_accuracy: 0.8769 - val_loss: 0.0699 - val_binary_accuracy: 0.8906 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0744 - binary_accuracy: 0.8743 - val_loss: 0.0861 - val_binary_accuracy: 0.8337 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 4s 12ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.83      0.90      2698\n",
            "           1       0.24      0.82      0.37       171\n",
            "\n",
            "    accuracy                           0.83      2869\n",
            "   macro avg       0.61      0.83      0.64      2869\n",
            "weighted avg       0.94      0.83      0.87      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2252  446]\n",
            " [  31  140]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6370586034062861"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "#model_PLSTM = build_paper_lstm(input_shape, num_classes)\n",
        "#model_CNNLSTM =  build_cnn_lstm(input_shape, num_classes)\n",
        "#model_BANET = build_banet_model()\n",
        "\n",
        "model_best = build_banet_model_late_test()\n",
        "y_pred, y_true, H, model = model_pipeline(model_best)\n",
        "plot_history(H)\n",
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uixd1qW-KZhe",
        "outputId": "103dfd89-04a2-4507-dce4-acb4f0aed926"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_run = 20\n",
        "best_F1 = 0\n",
        "for i in range(0, num_run):\n",
        "    model_best = build_banet_model_late_test()\n",
        "    y_pred, y_true, H, model = model_pipeline(model_best)\n",
        "    # plot_history(H)\n",
        "    f1_temp = f1_score(y_true, y_pred, average='macro')\n",
        "    if f1_temp > best_F1:\n",
        "        best_F1 = f1_temp\n",
        "        model.save_weights('/content/gdrive/MyDrive/Colab Notebooks/comp0053_dataset/' + 'model' + '.hdf5')\n",
        "\n",
        "print(best_F1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZiwpRE2P_My",
        "outputId": "76d0850a-26d7-4f0a-e03c-900df3cdd1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 397ms/step - loss: 0.1457 - binary_accuracy: 0.7207 - val_loss: 0.1401 - val_binary_accuracy: 0.9083 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1252 - binary_accuracy: 0.7899 - val_loss: 0.0980 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1186 - binary_accuracy: 0.8109 - val_loss: 0.0808 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1161 - binary_accuracy: 0.8142 - val_loss: 0.0743 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1141 - binary_accuracy: 0.8186 - val_loss: 0.0657 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1114 - binary_accuracy: 0.8216 - val_loss: 0.0642 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1118 - binary_accuracy: 0.8216 - val_loss: 0.0601 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1086 - binary_accuracy: 0.8242 - val_loss: 0.0616 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1082 - binary_accuracy: 0.8248 - val_loss: 0.0588 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1052 - binary_accuracy: 0.8252 - val_loss: 0.0605 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1046 - binary_accuracy: 0.8264 - val_loss: 0.0594 - val_binary_accuracy: 0.9418 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1010 - binary_accuracy: 0.8340 - val_loss: 0.0626 - val_binary_accuracy: 0.9411 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1008 - binary_accuracy: 0.8326 - val_loss: 0.0605 - val_binary_accuracy: 0.9407 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0988 - binary_accuracy: 0.8334 - val_loss: 0.0655 - val_binary_accuracy: 0.9407 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0989 - binary_accuracy: 0.8362 - val_loss: 0.0636 - val_binary_accuracy: 0.9411 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0965 - binary_accuracy: 0.8356 - val_loss: 0.0708 - val_binary_accuracy: 0.9202 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0959 - binary_accuracy: 0.8424 - val_loss: 0.0667 - val_binary_accuracy: 0.9272 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0928 - binary_accuracy: 0.8428 - val_loss: 0.0729 - val_binary_accuracy: 0.9069 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0924 - binary_accuracy: 0.8448 - val_loss: 0.0720 - val_binary_accuracy: 0.9010 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0895 - binary_accuracy: 0.8476 - val_loss: 0.0774 - val_binary_accuracy: 0.8986 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0893 - binary_accuracy: 0.8490 - val_loss: 0.0769 - val_binary_accuracy: 0.8951 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0874 - binary_accuracy: 0.8526 - val_loss: 0.0820 - val_binary_accuracy: 0.8913 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0862 - binary_accuracy: 0.8540 - val_loss: 0.0911 - val_binary_accuracy: 0.8724 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0848 - binary_accuracy: 0.8565 - val_loss: 0.0839 - val_binary_accuracy: 0.8839 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0842 - binary_accuracy: 0.8593 - val_loss: 0.1007 - val_binary_accuracy: 0.8578 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0836 - binary_accuracy: 0.8589 - val_loss: 0.0861 - val_binary_accuracy: 0.8811 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0830 - binary_accuracy: 0.8617 - val_loss: 0.1037 - val_binary_accuracy: 0.8498 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0817 - binary_accuracy: 0.8609 - val_loss: 0.0920 - val_binary_accuracy: 0.8777 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0805 - binary_accuracy: 0.8681 - val_loss: 0.1092 - val_binary_accuracy: 0.8463 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0810 - binary_accuracy: 0.8631 - val_loss: 0.0943 - val_binary_accuracy: 0.8710 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93      2698\n",
            "           1       0.27      0.69      0.39       171\n",
            "\n",
            "    accuracy                           0.87      2869\n",
            "   macro avg       0.62      0.79      0.66      2869\n",
            "weighted avg       0.94      0.87      0.90      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2381  317]\n",
            " [  53  118]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 26s 401ms/step - loss: 0.1465 - binary_accuracy: 0.7384 - val_loss: 0.1517 - val_binary_accuracy: 0.8606 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1278 - binary_accuracy: 0.7903 - val_loss: 0.1022 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1194 - binary_accuracy: 0.8113 - val_loss: 0.0868 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1158 - binary_accuracy: 0.8176 - val_loss: 0.0793 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1142 - binary_accuracy: 0.8188 - val_loss: 0.0735 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1121 - binary_accuracy: 0.8182 - val_loss: 0.0688 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1110 - binary_accuracy: 0.8212 - val_loss: 0.0660 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1079 - binary_accuracy: 0.8280 - val_loss: 0.0646 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1083 - binary_accuracy: 0.8232 - val_loss: 0.0652 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1054 - binary_accuracy: 0.8286 - val_loss: 0.0621 - val_binary_accuracy: 0.9453 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1037 - binary_accuracy: 0.8282 - val_loss: 0.0668 - val_binary_accuracy: 0.9439 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1014 - binary_accuracy: 0.8320 - val_loss: 0.0630 - val_binary_accuracy: 0.9435 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0990 - binary_accuracy: 0.8372 - val_loss: 0.0674 - val_binary_accuracy: 0.9383 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0985 - binary_accuracy: 0.8362 - val_loss: 0.0722 - val_binary_accuracy: 0.9317 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0958 - binary_accuracy: 0.8416 - val_loss: 0.0718 - val_binary_accuracy: 0.9313 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0953 - binary_accuracy: 0.8388 - val_loss: 0.0764 - val_binary_accuracy: 0.9132 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0925 - binary_accuracy: 0.8482 - val_loss: 0.0754 - val_binary_accuracy: 0.9090 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0912 - binary_accuracy: 0.8456 - val_loss: 0.0789 - val_binary_accuracy: 0.8982 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0901 - binary_accuracy: 0.8482 - val_loss: 0.0818 - val_binary_accuracy: 0.8895 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0887 - binary_accuracy: 0.8506 - val_loss: 0.0827 - val_binary_accuracy: 0.8913 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0875 - binary_accuracy: 0.8569 - val_loss: 0.0812 - val_binary_accuracy: 0.8867 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0857 - binary_accuracy: 0.8567 - val_loss: 0.0859 - val_binary_accuracy: 0.8825 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0840 - binary_accuracy: 0.8581 - val_loss: 0.0798 - val_binary_accuracy: 0.8892 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0829 - binary_accuracy: 0.8597 - val_loss: 0.0841 - val_binary_accuracy: 0.8871 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0819 - binary_accuracy: 0.8631 - val_loss: 0.0833 - val_binary_accuracy: 0.8853 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0808 - binary_accuracy: 0.8619 - val_loss: 0.0844 - val_binary_accuracy: 0.8832 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0804 - binary_accuracy: 0.8629 - val_loss: 0.0843 - val_binary_accuracy: 0.8829 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0797 - binary_accuracy: 0.8683 - val_loss: 0.0825 - val_binary_accuracy: 0.8804 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0785 - binary_accuracy: 0.8655 - val_loss: 0.0873 - val_binary_accuracy: 0.8710 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0782 - binary_accuracy: 0.8703 - val_loss: 0.0849 - val_binary_accuracy: 0.8756 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93      2698\n",
            "           1       0.27      0.64      0.38       171\n",
            "\n",
            "    accuracy                           0.88      2869\n",
            "   macro avg       0.62      0.77      0.66      2869\n",
            "weighted avg       0.93      0.88      0.90      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2402  296]\n",
            " [  61  110]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 26s 420ms/step - loss: 0.1556 - binary_accuracy: 0.7229 - val_loss: 0.1520 - val_binary_accuracy: 0.7954 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1329 - binary_accuracy: 0.7690 - val_loss: 0.0916 - val_binary_accuracy: 0.9289 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1210 - binary_accuracy: 0.8073 - val_loss: 0.0702 - val_binary_accuracy: 0.9449 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1162 - binary_accuracy: 0.8158 - val_loss: 0.0636 - val_binary_accuracy: 0.9446 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1134 - binary_accuracy: 0.8206 - val_loss: 0.0593 - val_binary_accuracy: 0.9456 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1111 - binary_accuracy: 0.8208 - val_loss: 0.0579 - val_binary_accuracy: 0.9446 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1092 - binary_accuracy: 0.8214 - val_loss: 0.0564 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.1074 - binary_accuracy: 0.8234 - val_loss: 0.0552 - val_binary_accuracy: 0.9446 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.1058 - binary_accuracy: 0.8218 - val_loss: 0.0549 - val_binary_accuracy: 0.9449 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1049 - binary_accuracy: 0.8256 - val_loss: 0.0569 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1027 - binary_accuracy: 0.8276 - val_loss: 0.0540 - val_binary_accuracy: 0.9428 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1019 - binary_accuracy: 0.8238 - val_loss: 0.0580 - val_binary_accuracy: 0.9421 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0999 - binary_accuracy: 0.8326 - val_loss: 0.0550 - val_binary_accuracy: 0.9421 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0986 - binary_accuracy: 0.8316 - val_loss: 0.0600 - val_binary_accuracy: 0.9355 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0959 - binary_accuracy: 0.8366 - val_loss: 0.0596 - val_binary_accuracy: 0.9265 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0945 - binary_accuracy: 0.8386 - val_loss: 0.0608 - val_binary_accuracy: 0.9240 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0936 - binary_accuracy: 0.8406 - val_loss: 0.0637 - val_binary_accuracy: 0.9125 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0915 - binary_accuracy: 0.8422 - val_loss: 0.0640 - val_binary_accuracy: 0.9080 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0907 - binary_accuracy: 0.8474 - val_loss: 0.0682 - val_binary_accuracy: 0.9031 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0889 - binary_accuracy: 0.8518 - val_loss: 0.0708 - val_binary_accuracy: 0.8982 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0870 - binary_accuracy: 0.8553 - val_loss: 0.0681 - val_binary_accuracy: 0.9014 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0866 - binary_accuracy: 0.8567 - val_loss: 0.0757 - val_binary_accuracy: 0.8881 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0852 - binary_accuracy: 0.8627 - val_loss: 0.0753 - val_binary_accuracy: 0.8902 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0837 - binary_accuracy: 0.8617 - val_loss: 0.0699 - val_binary_accuracy: 0.8972 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0835 - binary_accuracy: 0.8613 - val_loss: 0.0791 - val_binary_accuracy: 0.8791 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0818 - binary_accuracy: 0.8659 - val_loss: 0.0723 - val_binary_accuracy: 0.8930 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0819 - binary_accuracy: 0.8645 - val_loss: 0.0795 - val_binary_accuracy: 0.8770 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0807 - binary_accuracy: 0.8651 - val_loss: 0.0746 - val_binary_accuracy: 0.8933 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0800 - binary_accuracy: 0.8655 - val_loss: 0.0764 - val_binary_accuracy: 0.8846 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0792 - binary_accuracy: 0.8653 - val_loss: 0.0775 - val_binary_accuracy: 0.8899 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 4s 12ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      2698\n",
            "           1       0.31      0.71      0.44       171\n",
            "\n",
            "    accuracy                           0.89      2869\n",
            "   macro avg       0.65      0.81      0.69      2869\n",
            "weighted avg       0.94      0.89      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2431  267]\n",
            " [  49  122]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 26s 411ms/step - loss: 0.1508 - binary_accuracy: 0.7322 - val_loss: 0.1553 - val_binary_accuracy: 0.8034 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1309 - binary_accuracy: 0.7811 - val_loss: 0.1039 - val_binary_accuracy: 0.9310 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1242 - binary_accuracy: 0.7921 - val_loss: 0.0779 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1161 - binary_accuracy: 0.8208 - val_loss: 0.0717 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1155 - binary_accuracy: 0.8172 - val_loss: 0.0686 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1121 - binary_accuracy: 0.8216 - val_loss: 0.0642 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1125 - binary_accuracy: 0.8160 - val_loss: 0.0647 - val_binary_accuracy: 0.9400 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.1087 - binary_accuracy: 0.8210 - val_loss: 0.0601 - val_binary_accuracy: 0.9407 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.1089 - binary_accuracy: 0.8218 - val_loss: 0.0642 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1065 - binary_accuracy: 0.8246 - val_loss: 0.0595 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1064 - binary_accuracy: 0.8204 - val_loss: 0.0651 - val_binary_accuracy: 0.9376 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1032 - binary_accuracy: 0.8312 - val_loss: 0.0614 - val_binary_accuracy: 0.9428 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1028 - binary_accuracy: 0.8278 - val_loss: 0.0668 - val_binary_accuracy: 0.9310 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0997 - binary_accuracy: 0.8370 - val_loss: 0.0609 - val_binary_accuracy: 0.9418 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0996 - binary_accuracy: 0.8310 - val_loss: 0.0693 - val_binary_accuracy: 0.9198 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0970 - binary_accuracy: 0.8370 - val_loss: 0.0631 - val_binary_accuracy: 0.9348 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0967 - binary_accuracy: 0.8346 - val_loss: 0.0713 - val_binary_accuracy: 0.9115 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0944 - binary_accuracy: 0.8406 - val_loss: 0.0646 - val_binary_accuracy: 0.9233 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0934 - binary_accuracy: 0.8410 - val_loss: 0.0769 - val_binary_accuracy: 0.8895 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0915 - binary_accuracy: 0.8466 - val_loss: 0.0658 - val_binary_accuracy: 0.9181 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0910 - binary_accuracy: 0.8478 - val_loss: 0.0779 - val_binary_accuracy: 0.8749 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0891 - binary_accuracy: 0.8478 - val_loss: 0.0759 - val_binary_accuracy: 0.8860 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0876 - binary_accuracy: 0.8508 - val_loss: 0.0754 - val_binary_accuracy: 0.8773 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0867 - binary_accuracy: 0.8534 - val_loss: 0.0790 - val_binary_accuracy: 0.8756 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0854 - binary_accuracy: 0.8543 - val_loss: 0.0781 - val_binary_accuracy: 0.8749 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0842 - binary_accuracy: 0.8571 - val_loss: 0.0789 - val_binary_accuracy: 0.8773 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0837 - binary_accuracy: 0.8609 - val_loss: 0.0796 - val_binary_accuracy: 0.8766 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0827 - binary_accuracy: 0.8601 - val_loss: 0.0772 - val_binary_accuracy: 0.8780 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0812 - binary_accuracy: 0.8625 - val_loss: 0.0831 - val_binary_accuracy: 0.8675 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0807 - binary_accuracy: 0.8647 - val_loss: 0.0807 - val_binary_accuracy: 0.8675 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93      2698\n",
            "           1       0.28      0.75      0.40       171\n",
            "\n",
            "    accuracy                           0.87      2869\n",
            "   macro avg       0.63      0.81      0.66      2869\n",
            "weighted avg       0.94      0.87      0.89      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2361  337]\n",
            " [  43  128]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 26s 396ms/step - loss: 0.1437 - binary_accuracy: 0.7191 - val_loss: 0.1423 - val_binary_accuracy: 0.8261 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1280 - binary_accuracy: 0.7729 - val_loss: 0.0877 - val_binary_accuracy: 0.9348 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1209 - binary_accuracy: 0.8019 - val_loss: 0.0686 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1180 - binary_accuracy: 0.8101 - val_loss: 0.0595 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1157 - binary_accuracy: 0.8154 - val_loss: 0.0563 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1134 - binary_accuracy: 0.8174 - val_loss: 0.0537 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1104 - binary_accuracy: 0.8194 - val_loss: 0.0530 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1100 - binary_accuracy: 0.8194 - val_loss: 0.0520 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1072 - binary_accuracy: 0.8232 - val_loss: 0.0512 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1048 - binary_accuracy: 0.8270 - val_loss: 0.0516 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1015 - binary_accuracy: 0.8346 - val_loss: 0.0518 - val_binary_accuracy: 0.9362 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0992 - binary_accuracy: 0.8366 - val_loss: 0.0520 - val_binary_accuracy: 0.9366 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0970 - binary_accuracy: 0.8340 - val_loss: 0.0534 - val_binary_accuracy: 0.9355 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0954 - binary_accuracy: 0.8368 - val_loss: 0.0561 - val_binary_accuracy: 0.9324 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0926 - binary_accuracy: 0.8462 - val_loss: 0.0606 - val_binary_accuracy: 0.9268 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0905 - binary_accuracy: 0.8466 - val_loss: 0.0609 - val_binary_accuracy: 0.9261 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0885 - binary_accuracy: 0.8542 - val_loss: 0.0673 - val_binary_accuracy: 0.9132 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0875 - binary_accuracy: 0.8545 - val_loss: 0.0655 - val_binary_accuracy: 0.9132 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0862 - binary_accuracy: 0.8587 - val_loss: 0.0662 - val_binary_accuracy: 0.9080 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0850 - binary_accuracy: 0.8579 - val_loss: 0.0726 - val_binary_accuracy: 0.8864 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0820 - binary_accuracy: 0.8685 - val_loss: 0.0709 - val_binary_accuracy: 0.8857 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0812 - binary_accuracy: 0.8725 - val_loss: 0.0744 - val_binary_accuracy: 0.8829 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0808 - binary_accuracy: 0.8683 - val_loss: 0.0775 - val_binary_accuracy: 0.8693 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0789 - binary_accuracy: 0.8713 - val_loss: 0.0837 - val_binary_accuracy: 0.8616 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0788 - binary_accuracy: 0.8713 - val_loss: 0.0739 - val_binary_accuracy: 0.8745 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0769 - binary_accuracy: 0.8745 - val_loss: 0.0918 - val_binary_accuracy: 0.8452 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0755 - binary_accuracy: 0.8795 - val_loss: 0.0783 - val_binary_accuracy: 0.8714 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0743 - binary_accuracy: 0.8793 - val_loss: 0.0866 - val_binary_accuracy: 0.8543 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0746 - binary_accuracy: 0.8807 - val_loss: 0.0822 - val_binary_accuracy: 0.8637 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0727 - binary_accuracy: 0.8823 - val_loss: 0.0881 - val_binary_accuracy: 0.8567 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.86      0.92      2698\n",
            "           1       0.26      0.73      0.38       171\n",
            "\n",
            "    accuracy                           0.86      2869\n",
            "   macro avg       0.62      0.80      0.65      2869\n",
            "weighted avg       0.94      0.86      0.89      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2333  365]\n",
            " [  46  125]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 26s 524ms/step - loss: 0.1442 - binary_accuracy: 0.7484 - val_loss: 0.1518 - val_binary_accuracy: 0.8313 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1248 - binary_accuracy: 0.7959 - val_loss: 0.0976 - val_binary_accuracy: 0.9397 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1184 - binary_accuracy: 0.8121 - val_loss: 0.0787 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1148 - binary_accuracy: 0.8188 - val_loss: 0.0725 - val_binary_accuracy: 0.9446 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1133 - binary_accuracy: 0.8184 - val_loss: 0.0699 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1108 - binary_accuracy: 0.8214 - val_loss: 0.0667 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1095 - binary_accuracy: 0.8224 - val_loss: 0.0637 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1074 - binary_accuracy: 0.8268 - val_loss: 0.0610 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1066 - binary_accuracy: 0.8234 - val_loss: 0.0621 - val_binary_accuracy: 0.9453 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1038 - binary_accuracy: 0.8330 - val_loss: 0.0591 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1017 - binary_accuracy: 0.8330 - val_loss: 0.0591 - val_binary_accuracy: 0.9467 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0996 - binary_accuracy: 0.8360 - val_loss: 0.0587 - val_binary_accuracy: 0.9474 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0990 - binary_accuracy: 0.8364 - val_loss: 0.0587 - val_binary_accuracy: 0.9439 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0966 - binary_accuracy: 0.8358 - val_loss: 0.0599 - val_binary_accuracy: 0.9407 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0943 - binary_accuracy: 0.8430 - val_loss: 0.0582 - val_binary_accuracy: 0.9407 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0924 - binary_accuracy: 0.8448 - val_loss: 0.0594 - val_binary_accuracy: 0.9331 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0903 - binary_accuracy: 0.8456 - val_loss: 0.0594 - val_binary_accuracy: 0.9331 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0889 - binary_accuracy: 0.8464 - val_loss: 0.0584 - val_binary_accuracy: 0.9327 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0866 - binary_accuracy: 0.8536 - val_loss: 0.0598 - val_binary_accuracy: 0.9306 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0853 - binary_accuracy: 0.8571 - val_loss: 0.0619 - val_binary_accuracy: 0.9237 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0839 - binary_accuracy: 0.8609 - val_loss: 0.0614 - val_binary_accuracy: 0.9251 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0818 - binary_accuracy: 0.8655 - val_loss: 0.0666 - val_binary_accuracy: 0.9118 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0811 - binary_accuracy: 0.8681 - val_loss: 0.0677 - val_binary_accuracy: 0.9090 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0815 - binary_accuracy: 0.8659 - val_loss: 0.0674 - val_binary_accuracy: 0.9038 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0784 - binary_accuracy: 0.8729 - val_loss: 0.0657 - val_binary_accuracy: 0.9090 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0787 - binary_accuracy: 0.8697 - val_loss: 0.0713 - val_binary_accuracy: 0.8853 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0787 - binary_accuracy: 0.8679 - val_loss: 0.0727 - val_binary_accuracy: 0.8930 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0775 - binary_accuracy: 0.8737 - val_loss: 0.0742 - val_binary_accuracy: 0.8791 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0769 - binary_accuracy: 0.8725 - val_loss: 0.0753 - val_binary_accuracy: 0.8871 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0758 - binary_accuracy: 0.8787 - val_loss: 0.0728 - val_binary_accuracy: 0.8913 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.90      0.94      2698\n",
            "           1       0.33      0.80      0.47       171\n",
            "\n",
            "    accuracy                           0.89      2869\n",
            "   macro avg       0.66      0.85      0.70      2869\n",
            "weighted avg       0.95      0.89      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2421  277]\n",
            " [  35  136]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 397ms/step - loss: 0.1539 - binary_accuracy: 0.6939 - val_loss: 0.1573 - val_binary_accuracy: 0.7829 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1285 - binary_accuracy: 0.7817 - val_loss: 0.0954 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1205 - binary_accuracy: 0.8069 - val_loss: 0.0717 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1165 - binary_accuracy: 0.8130 - val_loss: 0.0639 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1141 - binary_accuracy: 0.8170 - val_loss: 0.0612 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.1121 - binary_accuracy: 0.8184 - val_loss: 0.0581 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1111 - binary_accuracy: 0.8194 - val_loss: 0.0561 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1085 - binary_accuracy: 0.8236 - val_loss: 0.0549 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1069 - binary_accuracy: 0.8230 - val_loss: 0.0545 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1063 - binary_accuracy: 0.8310 - val_loss: 0.0521 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1033 - binary_accuracy: 0.8262 - val_loss: 0.0536 - val_binary_accuracy: 0.9421 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1014 - binary_accuracy: 0.8342 - val_loss: 0.0528 - val_binary_accuracy: 0.9425 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0995 - binary_accuracy: 0.8330 - val_loss: 0.0542 - val_binary_accuracy: 0.9421 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0982 - binary_accuracy: 0.8420 - val_loss: 0.0527 - val_binary_accuracy: 0.9425 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0958 - binary_accuracy: 0.8402 - val_loss: 0.0537 - val_binary_accuracy: 0.9432 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0955 - binary_accuracy: 0.8446 - val_loss: 0.0544 - val_binary_accuracy: 0.9446 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0936 - binary_accuracy: 0.8452 - val_loss: 0.0554 - val_binary_accuracy: 0.9414 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0923 - binary_accuracy: 0.8472 - val_loss: 0.0557 - val_binary_accuracy: 0.9404 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0899 - binary_accuracy: 0.8488 - val_loss: 0.0564 - val_binary_accuracy: 0.9376 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0889 - binary_accuracy: 0.8577 - val_loss: 0.0597 - val_binary_accuracy: 0.9310 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0871 - binary_accuracy: 0.8540 - val_loss: 0.0591 - val_binary_accuracy: 0.9278 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0860 - binary_accuracy: 0.8607 - val_loss: 0.0609 - val_binary_accuracy: 0.9209 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0850 - binary_accuracy: 0.8593 - val_loss: 0.0668 - val_binary_accuracy: 0.9048 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0847 - binary_accuracy: 0.8657 - val_loss: 0.0606 - val_binary_accuracy: 0.9153 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0841 - binary_accuracy: 0.8621 - val_loss: 0.0746 - val_binary_accuracy: 0.8902 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0834 - binary_accuracy: 0.8651 - val_loss: 0.0594 - val_binary_accuracy: 0.9157 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0830 - binary_accuracy: 0.8637 - val_loss: 0.0802 - val_binary_accuracy: 0.8811 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0817 - binary_accuracy: 0.8653 - val_loss: 0.0661 - val_binary_accuracy: 0.9045 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0804 - binary_accuracy: 0.8643 - val_loss: 0.0792 - val_binary_accuracy: 0.8846 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0802 - binary_accuracy: 0.8675 - val_loss: 0.0712 - val_binary_accuracy: 0.8979 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94      2698\n",
            "           1       0.31      0.56      0.40       171\n",
            "\n",
            "    accuracy                           0.90      2869\n",
            "   macro avg       0.64      0.74      0.67      2869\n",
            "weighted avg       0.93      0.90      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2480  218]\n",
            " [  75   96]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 27s 399ms/step - loss: 0.1492 - binary_accuracy: 0.7386 - val_loss: 0.1539 - val_binary_accuracy: 0.8013 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1281 - binary_accuracy: 0.7847 - val_loss: 0.0969 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1206 - binary_accuracy: 0.8019 - val_loss: 0.0797 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1167 - binary_accuracy: 0.8144 - val_loss: 0.0714 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1138 - binary_accuracy: 0.8178 - val_loss: 0.0678 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1118 - binary_accuracy: 0.8162 - val_loss: 0.0637 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1099 - binary_accuracy: 0.8178 - val_loss: 0.0623 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1076 - binary_accuracy: 0.8212 - val_loss: 0.0576 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1055 - binary_accuracy: 0.8244 - val_loss: 0.0592 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1058 - binary_accuracy: 0.8236 - val_loss: 0.0551 - val_binary_accuracy: 0.9446 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1025 - binary_accuracy: 0.8328 - val_loss: 0.0593 - val_binary_accuracy: 0.9446 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1003 - binary_accuracy: 0.8298 - val_loss: 0.0595 - val_binary_accuracy: 0.9453 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0988 - binary_accuracy: 0.8338 - val_loss: 0.0565 - val_binary_accuracy: 0.9435 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0971 - binary_accuracy: 0.8372 - val_loss: 0.0613 - val_binary_accuracy: 0.9414 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0967 - binary_accuracy: 0.8360 - val_loss: 0.0589 - val_binary_accuracy: 0.9449 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0942 - binary_accuracy: 0.8370 - val_loss: 0.0646 - val_binary_accuracy: 0.9331 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0921 - binary_accuracy: 0.8444 - val_loss: 0.0596 - val_binary_accuracy: 0.9411 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0899 - binary_accuracy: 0.8512 - val_loss: 0.0621 - val_binary_accuracy: 0.9310 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0883 - binary_accuracy: 0.8480 - val_loss: 0.0665 - val_binary_accuracy: 0.9240 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0864 - binary_accuracy: 0.8522 - val_loss: 0.0635 - val_binary_accuracy: 0.9247 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0850 - binary_accuracy: 0.8538 - val_loss: 0.0691 - val_binary_accuracy: 0.9191 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0843 - binary_accuracy: 0.8553 - val_loss: 0.0659 - val_binary_accuracy: 0.9212 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0817 - binary_accuracy: 0.8623 - val_loss: 0.0754 - val_binary_accuracy: 0.9024 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0820 - binary_accuracy: 0.8609 - val_loss: 0.0712 - val_binary_accuracy: 0.9174 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0809 - binary_accuracy: 0.8613 - val_loss: 0.0810 - val_binary_accuracy: 0.8832 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0795 - binary_accuracy: 0.8659 - val_loss: 0.0748 - val_binary_accuracy: 0.9055 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0782 - binary_accuracy: 0.8705 - val_loss: 0.0852 - val_binary_accuracy: 0.8735 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0790 - binary_accuracy: 0.8715 - val_loss: 0.0763 - val_binary_accuracy: 0.8968 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0775 - binary_accuracy: 0.8731 - val_loss: 0.0825 - val_binary_accuracy: 0.8693 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0774 - binary_accuracy: 0.8739 - val_loss: 0.0757 - val_binary_accuracy: 0.8860 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      2698\n",
            "           1       0.30      0.70      0.42       171\n",
            "\n",
            "    accuracy                           0.89      2869\n",
            "   macro avg       0.64      0.80      0.68      2869\n",
            "weighted avg       0.94      0.89      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2423  275]\n",
            " [  52  119]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 400ms/step - loss: 0.1423 - binary_accuracy: 0.7666 - val_loss: 0.1472 - val_binary_accuracy: 0.8836 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1256 - binary_accuracy: 0.7839 - val_loss: 0.0952 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1177 - binary_accuracy: 0.8125 - val_loss: 0.0753 - val_binary_accuracy: 0.9453 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1156 - binary_accuracy: 0.8172 - val_loss: 0.0663 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1126 - binary_accuracy: 0.8212 - val_loss: 0.0615 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1117 - binary_accuracy: 0.8216 - val_loss: 0.0582 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1092 - binary_accuracy: 0.8242 - val_loss: 0.0575 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1088 - binary_accuracy: 0.8214 - val_loss: 0.0557 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1064 - binary_accuracy: 0.8258 - val_loss: 0.0545 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1048 - binary_accuracy: 0.8264 - val_loss: 0.0539 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1024 - binary_accuracy: 0.8314 - val_loss: 0.0532 - val_binary_accuracy: 0.9421 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1010 - binary_accuracy: 0.8282 - val_loss: 0.0541 - val_binary_accuracy: 0.9411 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0991 - binary_accuracy: 0.8378 - val_loss: 0.0531 - val_binary_accuracy: 0.9407 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0967 - binary_accuracy: 0.8364 - val_loss: 0.0534 - val_binary_accuracy: 0.9414 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0960 - binary_accuracy: 0.8390 - val_loss: 0.0546 - val_binary_accuracy: 0.9400 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0935 - binary_accuracy: 0.8430 - val_loss: 0.0543 - val_binary_accuracy: 0.9404 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0925 - binary_accuracy: 0.8434 - val_loss: 0.0536 - val_binary_accuracy: 0.9387 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0909 - binary_accuracy: 0.8458 - val_loss: 0.0556 - val_binary_accuracy: 0.9407 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0883 - binary_accuracy: 0.8498 - val_loss: 0.0586 - val_binary_accuracy: 0.9376 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0892 - binary_accuracy: 0.8538 - val_loss: 0.0526 - val_binary_accuracy: 0.9380 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0876 - binary_accuracy: 0.8530 - val_loss: 0.0591 - val_binary_accuracy: 0.9310 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0864 - binary_accuracy: 0.8530 - val_loss: 0.0592 - val_binary_accuracy: 0.9261 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0845 - binary_accuracy: 0.8603 - val_loss: 0.0628 - val_binary_accuracy: 0.9251 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0832 - binary_accuracy: 0.8635 - val_loss: 0.0645 - val_binary_accuracy: 0.9153 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0818 - binary_accuracy: 0.8627 - val_loss: 0.0623 - val_binary_accuracy: 0.9202 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0809 - binary_accuracy: 0.8677 - val_loss: 0.0732 - val_binary_accuracy: 0.9080 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0800 - binary_accuracy: 0.8687 - val_loss: 0.0664 - val_binary_accuracy: 0.9118 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0789 - binary_accuracy: 0.8715 - val_loss: 0.0715 - val_binary_accuracy: 0.9094 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0783 - binary_accuracy: 0.8711 - val_loss: 0.0724 - val_binary_accuracy: 0.9017 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0770 - binary_accuracy: 0.8751 - val_loss: 0.0705 - val_binary_accuracy: 0.9101 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      2698\n",
            "           1       0.35      0.58      0.44       171\n",
            "\n",
            "    accuracy                           0.91      2869\n",
            "   macro avg       0.66      0.76      0.69      2869\n",
            "weighted avg       0.94      0.91      0.92      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2511  187]\n",
            " [  71  100]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 28s 412ms/step - loss: 0.1402 - binary_accuracy: 0.7674 - val_loss: 0.1450 - val_binary_accuracy: 0.8547 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1237 - binary_accuracy: 0.7953 - val_loss: 0.0918 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1184 - binary_accuracy: 0.8128 - val_loss: 0.0749 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1158 - binary_accuracy: 0.8160 - val_loss: 0.0662 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1129 - binary_accuracy: 0.8202 - val_loss: 0.0600 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1108 - binary_accuracy: 0.8196 - val_loss: 0.0579 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1090 - binary_accuracy: 0.8254 - val_loss: 0.0557 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1075 - binary_accuracy: 0.8232 - val_loss: 0.0543 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1060 - binary_accuracy: 0.8248 - val_loss: 0.0532 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1035 - binary_accuracy: 0.8310 - val_loss: 0.0523 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1035 - binary_accuracy: 0.8286 - val_loss: 0.0524 - val_binary_accuracy: 0.9421 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1013 - binary_accuracy: 0.8338 - val_loss: 0.0524 - val_binary_accuracy: 0.9418 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0994 - binary_accuracy: 0.8350 - val_loss: 0.0523 - val_binary_accuracy: 0.9407 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0980 - binary_accuracy: 0.8376 - val_loss: 0.0514 - val_binary_accuracy: 0.9418 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0967 - binary_accuracy: 0.8392 - val_loss: 0.0535 - val_binary_accuracy: 0.9421 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0951 - binary_accuracy: 0.8448 - val_loss: 0.0524 - val_binary_accuracy: 0.9428 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0945 - binary_accuracy: 0.8434 - val_loss: 0.0552 - val_binary_accuracy: 0.9296 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0930 - binary_accuracy: 0.8470 - val_loss: 0.0518 - val_binary_accuracy: 0.9400 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0910 - binary_accuracy: 0.8488 - val_loss: 0.0565 - val_binary_accuracy: 0.9202 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0900 - binary_accuracy: 0.8464 - val_loss: 0.0545 - val_binary_accuracy: 0.9275 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0887 - binary_accuracy: 0.8510 - val_loss: 0.0569 - val_binary_accuracy: 0.9108 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0882 - binary_accuracy: 0.8520 - val_loss: 0.0565 - val_binary_accuracy: 0.9139 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0864 - binary_accuracy: 0.8540 - val_loss: 0.0567 - val_binary_accuracy: 0.9108 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0854 - binary_accuracy: 0.8589 - val_loss: 0.0606 - val_binary_accuracy: 0.9048 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0848 - binary_accuracy: 0.8609 - val_loss: 0.0592 - val_binary_accuracy: 0.9080 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0835 - binary_accuracy: 0.8593 - val_loss: 0.0613 - val_binary_accuracy: 0.9031 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0825 - binary_accuracy: 0.8653 - val_loss: 0.0600 - val_binary_accuracy: 0.9073 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0809 - binary_accuracy: 0.8635 - val_loss: 0.0629 - val_binary_accuracy: 0.9052 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0807 - binary_accuracy: 0.8629 - val_loss: 0.0616 - val_binary_accuracy: 0.9069 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0807 - binary_accuracy: 0.8659 - val_loss: 0.0680 - val_binary_accuracy: 0.8982 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94      2698\n",
            "           1       0.31      0.56      0.40       171\n",
            "\n",
            "    accuracy                           0.90      2869\n",
            "   macro avg       0.64      0.74      0.67      2869\n",
            "weighted avg       0.93      0.90      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2481  217]\n",
            " [  75   96]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 397ms/step - loss: 0.1446 - binary_accuracy: 0.7322 - val_loss: 0.1506 - val_binary_accuracy: 0.8418 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1222 - binary_accuracy: 0.8013 - val_loss: 0.1025 - val_binary_accuracy: 0.9397 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1178 - binary_accuracy: 0.8113 - val_loss: 0.0808 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1153 - binary_accuracy: 0.8180 - val_loss: 0.0715 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1138 - binary_accuracy: 0.8162 - val_loss: 0.0643 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1118 - binary_accuracy: 0.8208 - val_loss: 0.0601 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1099 - binary_accuracy: 0.8212 - val_loss: 0.0577 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1089 - binary_accuracy: 0.8252 - val_loss: 0.0549 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1071 - binary_accuracy: 0.8246 - val_loss: 0.0549 - val_binary_accuracy: 0.9439 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1056 - binary_accuracy: 0.8292 - val_loss: 0.0542 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1034 - binary_accuracy: 0.8330 - val_loss: 0.0548 - val_binary_accuracy: 0.9418 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1046 - binary_accuracy: 0.8312 - val_loss: 0.0532 - val_binary_accuracy: 0.9411 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1010 - binary_accuracy: 0.8364 - val_loss: 0.0538 - val_binary_accuracy: 0.9394 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1006 - binary_accuracy: 0.8340 - val_loss: 0.0561 - val_binary_accuracy: 0.9373 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0975 - binary_accuracy: 0.8396 - val_loss: 0.0557 - val_binary_accuracy: 0.9310 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0970 - binary_accuracy: 0.8392 - val_loss: 0.0591 - val_binary_accuracy: 0.9285 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0940 - binary_accuracy: 0.8450 - val_loss: 0.0577 - val_binary_accuracy: 0.9289 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0928 - binary_accuracy: 0.8436 - val_loss: 0.0582 - val_binary_accuracy: 0.9282 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0914 - binary_accuracy: 0.8466 - val_loss: 0.0622 - val_binary_accuracy: 0.9223 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0890 - binary_accuracy: 0.8504 - val_loss: 0.0625 - val_binary_accuracy: 0.9202 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0879 - binary_accuracy: 0.8553 - val_loss: 0.0657 - val_binary_accuracy: 0.9108 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0869 - binary_accuracy: 0.8597 - val_loss: 0.0708 - val_binary_accuracy: 0.9048 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0849 - binary_accuracy: 0.8563 - val_loss: 0.0623 - val_binary_accuracy: 0.9139 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0846 - binary_accuracy: 0.8601 - val_loss: 0.0697 - val_binary_accuracy: 0.8979 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0831 - binary_accuracy: 0.8627 - val_loss: 0.0701 - val_binary_accuracy: 0.8923 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0815 - binary_accuracy: 0.8621 - val_loss: 0.0658 - val_binary_accuracy: 0.8958 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0813 - binary_accuracy: 0.8659 - val_loss: 0.0751 - val_binary_accuracy: 0.8846 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0796 - binary_accuracy: 0.8665 - val_loss: 0.0730 - val_binary_accuracy: 0.8853 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0797 - binary_accuracy: 0.8641 - val_loss: 0.0658 - val_binary_accuracy: 0.8965 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0781 - binary_accuracy: 0.8681 - val_loss: 0.0795 - val_binary_accuracy: 0.8822 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.93      2698\n",
            "           1       0.31      0.81      0.45       171\n",
            "\n",
            "    accuracy                           0.88      2869\n",
            "   macro avg       0.65      0.85      0.69      2869\n",
            "weighted avg       0.95      0.88      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2393  305]\n",
            " [  33  138]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 392ms/step - loss: 0.1521 - binary_accuracy: 0.7452 - val_loss: 0.1468 - val_binary_accuracy: 0.8271 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1286 - binary_accuracy: 0.7861 - val_loss: 0.0896 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1226 - binary_accuracy: 0.8027 - val_loss: 0.0727 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1171 - binary_accuracy: 0.8166 - val_loss: 0.0691 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1144 - binary_accuracy: 0.8184 - val_loss: 0.0654 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1125 - binary_accuracy: 0.8198 - val_loss: 0.0652 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1111 - binary_accuracy: 0.8234 - val_loss: 0.0599 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1090 - binary_accuracy: 0.8260 - val_loss: 0.0652 - val_binary_accuracy: 0.9456 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1077 - binary_accuracy: 0.8278 - val_loss: 0.0599 - val_binary_accuracy: 0.9449 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1060 - binary_accuracy: 0.8282 - val_loss: 0.0645 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1048 - binary_accuracy: 0.8300 - val_loss: 0.0589 - val_binary_accuracy: 0.9439 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1021 - binary_accuracy: 0.8314 - val_loss: 0.0645 - val_binary_accuracy: 0.9387 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1025 - binary_accuracy: 0.8308 - val_loss: 0.0595 - val_binary_accuracy: 0.9418 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0986 - binary_accuracy: 0.8384 - val_loss: 0.0647 - val_binary_accuracy: 0.9359 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0985 - binary_accuracy: 0.8364 - val_loss: 0.0663 - val_binary_accuracy: 0.9348 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0955 - binary_accuracy: 0.8404 - val_loss: 0.0655 - val_binary_accuracy: 0.9338 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0943 - binary_accuracy: 0.8384 - val_loss: 0.0684 - val_binary_accuracy: 0.9230 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0922 - binary_accuracy: 0.8446 - val_loss: 0.0714 - val_binary_accuracy: 0.9153 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0903 - binary_accuracy: 0.8478 - val_loss: 0.0706 - val_binary_accuracy: 0.9101 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0896 - binary_accuracy: 0.8476 - val_loss: 0.0730 - val_binary_accuracy: 0.9010 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0884 - binary_accuracy: 0.8512 - val_loss: 0.0758 - val_binary_accuracy: 0.8975 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0869 - binary_accuracy: 0.8540 - val_loss: 0.0762 - val_binary_accuracy: 0.8923 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0865 - binary_accuracy: 0.8534 - val_loss: 0.0819 - val_binary_accuracy: 0.8874 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0847 - binary_accuracy: 0.8565 - val_loss: 0.0756 - val_binary_accuracy: 0.8930 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0831 - binary_accuracy: 0.8567 - val_loss: 0.0830 - val_binary_accuracy: 0.8860 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0831 - binary_accuracy: 0.8607 - val_loss: 0.0788 - val_binary_accuracy: 0.8860 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0814 - binary_accuracy: 0.8623 - val_loss: 0.0795 - val_binary_accuracy: 0.8913 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0806 - binary_accuracy: 0.8643 - val_loss: 0.0828 - val_binary_accuracy: 0.8815 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0795 - binary_accuracy: 0.8671 - val_loss: 0.0818 - val_binary_accuracy: 0.8808 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0797 - binary_accuracy: 0.8675 - val_loss: 0.0909 - val_binary_accuracy: 0.8707 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93      2698\n",
            "           1       0.28      0.77      0.41       171\n",
            "\n",
            "    accuracy                           0.87      2869\n",
            "   macro avg       0.63      0.82      0.67      2869\n",
            "weighted avg       0.94      0.87      0.90      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2367  331]\n",
            " [  40  131]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 28s 393ms/step - loss: 0.1531 - binary_accuracy: 0.7113 - val_loss: 0.1603 - val_binary_accuracy: 0.7400 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1309 - binary_accuracy: 0.7753 - val_loss: 0.1030 - val_binary_accuracy: 0.9230 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1211 - binary_accuracy: 0.7983 - val_loss: 0.0798 - val_binary_accuracy: 0.9394 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1171 - binary_accuracy: 0.8123 - val_loss: 0.0700 - val_binary_accuracy: 0.9407 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1137 - binary_accuracy: 0.8166 - val_loss: 0.0667 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1130 - binary_accuracy: 0.8226 - val_loss: 0.0632 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1108 - binary_accuracy: 0.8232 - val_loss: 0.0634 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1104 - binary_accuracy: 0.8242 - val_loss: 0.0622 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1084 - binary_accuracy: 0.8256 - val_loss: 0.0615 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1069 - binary_accuracy: 0.8276 - val_loss: 0.0615 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1054 - binary_accuracy: 0.8286 - val_loss: 0.0621 - val_binary_accuracy: 0.9425 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1047 - binary_accuracy: 0.8292 - val_loss: 0.0595 - val_binary_accuracy: 0.9418 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1034 - binary_accuracy: 0.8294 - val_loss: 0.0665 - val_binary_accuracy: 0.9320 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1004 - binary_accuracy: 0.8366 - val_loss: 0.0638 - val_binary_accuracy: 0.9345 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0995 - binary_accuracy: 0.8304 - val_loss: 0.0685 - val_binary_accuracy: 0.9212 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0979 - binary_accuracy: 0.8352 - val_loss: 0.0668 - val_binary_accuracy: 0.9272 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0962 - binary_accuracy: 0.8348 - val_loss: 0.0726 - val_binary_accuracy: 0.9118 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0956 - binary_accuracy: 0.8392 - val_loss: 0.0704 - val_binary_accuracy: 0.9163 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0936 - binary_accuracy: 0.8374 - val_loss: 0.0734 - val_binary_accuracy: 0.9076 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0925 - binary_accuracy: 0.8444 - val_loss: 0.0686 - val_binary_accuracy: 0.9184 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0912 - binary_accuracy: 0.8420 - val_loss: 0.0813 - val_binary_accuracy: 0.8815 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0902 - binary_accuracy: 0.8470 - val_loss: 0.0745 - val_binary_accuracy: 0.9041 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0890 - binary_accuracy: 0.8466 - val_loss: 0.0830 - val_binary_accuracy: 0.8710 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0882 - binary_accuracy: 0.8464 - val_loss: 0.0724 - val_binary_accuracy: 0.8979 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0868 - binary_accuracy: 0.8498 - val_loss: 0.0804 - val_binary_accuracy: 0.8780 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0853 - binary_accuracy: 0.8536 - val_loss: 0.0753 - val_binary_accuracy: 0.8878 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0844 - binary_accuracy: 0.8522 - val_loss: 0.0786 - val_binary_accuracy: 0.8797 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0841 - binary_accuracy: 0.8536 - val_loss: 0.0769 - val_binary_accuracy: 0.8808 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0834 - binary_accuracy: 0.8563 - val_loss: 0.0780 - val_binary_accuracy: 0.8808 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0835 - binary_accuracy: 0.8553 - val_loss: 0.0816 - val_binary_accuracy: 0.8728 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 12ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.88      0.93      2698\n",
            "           1       0.29      0.79      0.43       171\n",
            "\n",
            "    accuracy                           0.87      2869\n",
            "   macro avg       0.64      0.83      0.68      2869\n",
            "weighted avg       0.94      0.87      0.90      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2369  329]\n",
            " [  36  135]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 395ms/step - loss: 0.1520 - binary_accuracy: 0.7091 - val_loss: 0.1578 - val_binary_accuracy: 0.7483 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1286 - binary_accuracy: 0.7897 - val_loss: 0.0964 - val_binary_accuracy: 0.9296 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1215 - binary_accuracy: 0.8051 - val_loss: 0.0726 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1179 - binary_accuracy: 0.8130 - val_loss: 0.0647 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1144 - binary_accuracy: 0.8176 - val_loss: 0.0627 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1124 - binary_accuracy: 0.8210 - val_loss: 0.0575 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.1106 - binary_accuracy: 0.8226 - val_loss: 0.0590 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1102 - binary_accuracy: 0.8226 - val_loss: 0.0556 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1072 - binary_accuracy: 0.8272 - val_loss: 0.0561 - val_binary_accuracy: 0.9390 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1069 - binary_accuracy: 0.8238 - val_loss: 0.0552 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1047 - binary_accuracy: 0.8270 - val_loss: 0.0545 - val_binary_accuracy: 0.9418 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1023 - binary_accuracy: 0.8310 - val_loss: 0.0564 - val_binary_accuracy: 0.9407 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1018 - binary_accuracy: 0.8300 - val_loss: 0.0545 - val_binary_accuracy: 0.9428 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1000 - binary_accuracy: 0.8326 - val_loss: 0.0565 - val_binary_accuracy: 0.9432 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0990 - binary_accuracy: 0.8320 - val_loss: 0.0600 - val_binary_accuracy: 0.9334 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0977 - binary_accuracy: 0.8394 - val_loss: 0.0598 - val_binary_accuracy: 0.9327 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0956 - binary_accuracy: 0.8418 - val_loss: 0.0589 - val_binary_accuracy: 0.9331 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0945 - binary_accuracy: 0.8376 - val_loss: 0.0607 - val_binary_accuracy: 0.9265 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0931 - binary_accuracy: 0.8430 - val_loss: 0.0592 - val_binary_accuracy: 0.9254 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0923 - binary_accuracy: 0.8436 - val_loss: 0.0617 - val_binary_accuracy: 0.9184 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0911 - binary_accuracy: 0.8442 - val_loss: 0.0613 - val_binary_accuracy: 0.9115 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0901 - binary_accuracy: 0.8490 - val_loss: 0.0620 - val_binary_accuracy: 0.9080 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0899 - binary_accuracy: 0.8474 - val_loss: 0.0640 - val_binary_accuracy: 0.8996 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0876 - binary_accuracy: 0.8542 - val_loss: 0.0641 - val_binary_accuracy: 0.9010 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0866 - binary_accuracy: 0.8536 - val_loss: 0.0664 - val_binary_accuracy: 0.8933 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0860 - binary_accuracy: 0.8545 - val_loss: 0.0660 - val_binary_accuracy: 0.8954 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0844 - binary_accuracy: 0.8585 - val_loss: 0.0661 - val_binary_accuracy: 0.8919 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0847 - binary_accuracy: 0.8585 - val_loss: 0.0639 - val_binary_accuracy: 0.8979 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0837 - binary_accuracy: 0.8637 - val_loss: 0.0652 - val_binary_accuracy: 0.8975 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0830 - binary_accuracy: 0.8665 - val_loss: 0.0656 - val_binary_accuracy: 0.8961 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      2698\n",
            "           1       0.31      0.62      0.42       171\n",
            "\n",
            "    accuracy                           0.90      2869\n",
            "   macro avg       0.64      0.77      0.68      2869\n",
            "weighted avg       0.93      0.90      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2465  233]\n",
            " [  65  106]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 396ms/step - loss: 0.1486 - binary_accuracy: 0.7328 - val_loss: 0.1496 - val_binary_accuracy: 0.8181 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1298 - binary_accuracy: 0.7857 - val_loss: 0.0911 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1231 - binary_accuracy: 0.8037 - val_loss: 0.0754 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1186 - binary_accuracy: 0.8142 - val_loss: 0.0683 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1159 - binary_accuracy: 0.8194 - val_loss: 0.0637 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1124 - binary_accuracy: 0.8210 - val_loss: 0.0605 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1118 - binary_accuracy: 0.8174 - val_loss: 0.0595 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1092 - binary_accuracy: 0.8202 - val_loss: 0.0571 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1082 - binary_accuracy: 0.8220 - val_loss: 0.0562 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1059 - binary_accuracy: 0.8262 - val_loss: 0.0545 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1040 - binary_accuracy: 0.8324 - val_loss: 0.0555 - val_binary_accuracy: 0.9407 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1023 - binary_accuracy: 0.8316 - val_loss: 0.0530 - val_binary_accuracy: 0.9428 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0993 - binary_accuracy: 0.8380 - val_loss: 0.0543 - val_binary_accuracy: 0.9411 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0979 - binary_accuracy: 0.8392 - val_loss: 0.0514 - val_binary_accuracy: 0.9397 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0948 - binary_accuracy: 0.8454 - val_loss: 0.0509 - val_binary_accuracy: 0.9400 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0949 - binary_accuracy: 0.8466 - val_loss: 0.0498 - val_binary_accuracy: 0.9397 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0913 - binary_accuracy: 0.8476 - val_loss: 0.0504 - val_binary_accuracy: 0.9407 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0902 - binary_accuracy: 0.8555 - val_loss: 0.0500 - val_binary_accuracy: 0.9366 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0870 - binary_accuracy: 0.8597 - val_loss: 0.0503 - val_binary_accuracy: 0.9362 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0860 - binary_accuracy: 0.8621 - val_loss: 0.0518 - val_binary_accuracy: 0.9324 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0836 - binary_accuracy: 0.8627 - val_loss: 0.0498 - val_binary_accuracy: 0.9355 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0832 - binary_accuracy: 0.8659 - val_loss: 0.0521 - val_binary_accuracy: 0.9289 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0817 - binary_accuracy: 0.8703 - val_loss: 0.0518 - val_binary_accuracy: 0.9289 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0792 - binary_accuracy: 0.8759 - val_loss: 0.0543 - val_binary_accuracy: 0.9289 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0798 - binary_accuracy: 0.8723 - val_loss: 0.0570 - val_binary_accuracy: 0.9272 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0782 - binary_accuracy: 0.8719 - val_loss: 0.0574 - val_binary_accuracy: 0.9285 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0767 - binary_accuracy: 0.8795 - val_loss: 0.0600 - val_binary_accuracy: 0.9265 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0763 - binary_accuracy: 0.8737 - val_loss: 0.0622 - val_binary_accuracy: 0.9188 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0753 - binary_accuracy: 0.8817 - val_loss: 0.0578 - val_binary_accuracy: 0.9289 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0743 - binary_accuracy: 0.8827 - val_loss: 0.0638 - val_binary_accuracy: 0.9035 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95      2698\n",
            "           1       0.35      0.74      0.48       171\n",
            "\n",
            "    accuracy                           0.90      2869\n",
            "   macro avg       0.67      0.83      0.71      2869\n",
            "weighted avg       0.94      0.90      0.92      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2466  232]\n",
            " [  45  126]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 28s 395ms/step - loss: 0.1522 - binary_accuracy: 0.7139 - val_loss: 0.1470 - val_binary_accuracy: 0.8578 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1290 - binary_accuracy: 0.7889 - val_loss: 0.0991 - val_binary_accuracy: 0.9369 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1204 - binary_accuracy: 0.8069 - val_loss: 0.0806 - val_binary_accuracy: 0.9442 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1157 - binary_accuracy: 0.8198 - val_loss: 0.0755 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1134 - binary_accuracy: 0.8186 - val_loss: 0.0695 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1111 - binary_accuracy: 0.8174 - val_loss: 0.0663 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1098 - binary_accuracy: 0.8218 - val_loss: 0.0631 - val_binary_accuracy: 0.9453 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.1086 - binary_accuracy: 0.8224 - val_loss: 0.0663 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1077 - binary_accuracy: 0.8222 - val_loss: 0.0607 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1051 - binary_accuracy: 0.8274 - val_loss: 0.0622 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1043 - binary_accuracy: 0.8288 - val_loss: 0.0589 - val_binary_accuracy: 0.9418 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1019 - binary_accuracy: 0.8316 - val_loss: 0.0626 - val_binary_accuracy: 0.9407 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1008 - binary_accuracy: 0.8308 - val_loss: 0.0612 - val_binary_accuracy: 0.9442 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0987 - binary_accuracy: 0.8356 - val_loss: 0.0621 - val_binary_accuracy: 0.9449 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0987 - binary_accuracy: 0.8324 - val_loss: 0.0634 - val_binary_accuracy: 0.9425 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0968 - binary_accuracy: 0.8388 - val_loss: 0.0604 - val_binary_accuracy: 0.9411 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0953 - binary_accuracy: 0.8364 - val_loss: 0.0642 - val_binary_accuracy: 0.9282 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0929 - binary_accuracy: 0.8434 - val_loss: 0.0658 - val_binary_accuracy: 0.9153 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0919 - binary_accuracy: 0.8438 - val_loss: 0.0652 - val_binary_accuracy: 0.9122 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0908 - binary_accuracy: 0.8476 - val_loss: 0.0710 - val_binary_accuracy: 0.8951 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0903 - binary_accuracy: 0.8438 - val_loss: 0.0685 - val_binary_accuracy: 0.8989 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0886 - binary_accuracy: 0.8508 - val_loss: 0.0763 - val_binary_accuracy: 0.8881 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0876 - binary_accuracy: 0.8516 - val_loss: 0.0727 - val_binary_accuracy: 0.8902 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0863 - binary_accuracy: 0.8555 - val_loss: 0.0786 - val_binary_accuracy: 0.8864 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0863 - binary_accuracy: 0.8542 - val_loss: 0.0792 - val_binary_accuracy: 0.8857 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0849 - binary_accuracy: 0.8599 - val_loss: 0.0767 - val_binary_accuracy: 0.8874 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0847 - binary_accuracy: 0.8611 - val_loss: 0.0860 - val_binary_accuracy: 0.8763 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0834 - binary_accuracy: 0.8623 - val_loss: 0.0802 - val_binary_accuracy: 0.8829 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0830 - binary_accuracy: 0.8617 - val_loss: 0.0878 - val_binary_accuracy: 0.8721 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0826 - binary_accuracy: 0.8609 - val_loss: 0.0850 - val_binary_accuracy: 0.8763 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93      2698\n",
            "           1       0.28      0.67      0.39       171\n",
            "\n",
            "    accuracy                           0.88      2869\n",
            "   macro avg       0.63      0.78      0.66      2869\n",
            "weighted avg       0.94      0.88      0.90      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2399  299]\n",
            " [  56  115]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 401ms/step - loss: 0.1534 - binary_accuracy: 0.6852 - val_loss: 0.1543 - val_binary_accuracy: 0.7919 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1290 - binary_accuracy: 0.7795 - val_loss: 0.1069 - val_binary_accuracy: 0.9338 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1219 - binary_accuracy: 0.8017 - val_loss: 0.0814 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1165 - binary_accuracy: 0.8121 - val_loss: 0.0724 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1146 - binary_accuracy: 0.8111 - val_loss: 0.0668 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1111 - binary_accuracy: 0.8184 - val_loss: 0.0631 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1112 - binary_accuracy: 0.8176 - val_loss: 0.0606 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1072 - binary_accuracy: 0.8258 - val_loss: 0.0595 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1075 - binary_accuracy: 0.8212 - val_loss: 0.0577 - val_binary_accuracy: 0.9435 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1049 - binary_accuracy: 0.8258 - val_loss: 0.0595 - val_binary_accuracy: 0.9397 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1058 - binary_accuracy: 0.8254 - val_loss: 0.0549 - val_binary_accuracy: 0.9421 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1015 - binary_accuracy: 0.8356 - val_loss: 0.0553 - val_binary_accuracy: 0.9435 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1010 - binary_accuracy: 0.8318 - val_loss: 0.0579 - val_binary_accuracy: 0.9421 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0988 - binary_accuracy: 0.8332 - val_loss: 0.0570 - val_binary_accuracy: 0.9439 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0971 - binary_accuracy: 0.8342 - val_loss: 0.0580 - val_binary_accuracy: 0.9414 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0950 - binary_accuracy: 0.8390 - val_loss: 0.0599 - val_binary_accuracy: 0.9432 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0940 - binary_accuracy: 0.8378 - val_loss: 0.0618 - val_binary_accuracy: 0.9387 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0919 - binary_accuracy: 0.8408 - val_loss: 0.0613 - val_binary_accuracy: 0.9359 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0909 - binary_accuracy: 0.8458 - val_loss: 0.0648 - val_binary_accuracy: 0.9244 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0895 - binary_accuracy: 0.8464 - val_loss: 0.0677 - val_binary_accuracy: 0.9230 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0880 - binary_accuracy: 0.8549 - val_loss: 0.0663 - val_binary_accuracy: 0.9237 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0869 - binary_accuracy: 0.8555 - val_loss: 0.0731 - val_binary_accuracy: 0.9087 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0862 - binary_accuracy: 0.8542 - val_loss: 0.0755 - val_binary_accuracy: 0.9031 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0846 - binary_accuracy: 0.8603 - val_loss: 0.0759 - val_binary_accuracy: 0.8961 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0834 - binary_accuracy: 0.8583 - val_loss: 0.0799 - val_binary_accuracy: 0.8923 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0835 - binary_accuracy: 0.8583 - val_loss: 0.0751 - val_binary_accuracy: 0.8965 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0814 - binary_accuracy: 0.8611 - val_loss: 0.0840 - val_binary_accuracy: 0.8777 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0821 - binary_accuracy: 0.8565 - val_loss: 0.0763 - val_binary_accuracy: 0.8940 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0811 - binary_accuracy: 0.8603 - val_loss: 0.0901 - val_binary_accuracy: 0.8731 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0807 - binary_accuracy: 0.8647 - val_loss: 0.0810 - val_binary_accuracy: 0.8832 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.93      2698\n",
            "           1       0.31      0.79      0.45       171\n",
            "\n",
            "    accuracy                           0.88      2869\n",
            "   macro avg       0.65      0.84      0.69      2869\n",
            "weighted avg       0.95      0.88      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2399  299]\n",
            " [  36  135]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 405ms/step - loss: 0.1474 - binary_accuracy: 0.6933 - val_loss: 0.1470 - val_binary_accuracy: 0.9209 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1262 - binary_accuracy: 0.7881 - val_loss: 0.0983 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1189 - binary_accuracy: 0.8142 - val_loss: 0.0797 - val_binary_accuracy: 0.9418 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1159 - binary_accuracy: 0.8180 - val_loss: 0.0719 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1147 - binary_accuracy: 0.8176 - val_loss: 0.0688 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1124 - binary_accuracy: 0.8224 - val_loss: 0.0671 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1115 - binary_accuracy: 0.8206 - val_loss: 0.0618 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1101 - binary_accuracy: 0.8204 - val_loss: 0.0616 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1085 - binary_accuracy: 0.8210 - val_loss: 0.0593 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1063 - binary_accuracy: 0.8232 - val_loss: 0.0600 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1049 - binary_accuracy: 0.8250 - val_loss: 0.0592 - val_binary_accuracy: 0.9400 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1035 - binary_accuracy: 0.8256 - val_loss: 0.0604 - val_binary_accuracy: 0.9400 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1018 - binary_accuracy: 0.8276 - val_loss: 0.0608 - val_binary_accuracy: 0.9400 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0984 - binary_accuracy: 0.8304 - val_loss: 0.0623 - val_binary_accuracy: 0.9404 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0995 - binary_accuracy: 0.8304 - val_loss: 0.0597 - val_binary_accuracy: 0.9376 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0971 - binary_accuracy: 0.8350 - val_loss: 0.0615 - val_binary_accuracy: 0.9334 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0969 - binary_accuracy: 0.8334 - val_loss: 0.0625 - val_binary_accuracy: 0.9282 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0948 - binary_accuracy: 0.8400 - val_loss: 0.0640 - val_binary_accuracy: 0.9237 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0930 - binary_accuracy: 0.8432 - val_loss: 0.0638 - val_binary_accuracy: 0.9244 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0912 - binary_accuracy: 0.8462 - val_loss: 0.0661 - val_binary_accuracy: 0.9198 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0905 - binary_accuracy: 0.8458 - val_loss: 0.0657 - val_binary_accuracy: 0.9177 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0889 - binary_accuracy: 0.8530 - val_loss: 0.0666 - val_binary_accuracy: 0.9122 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0883 - binary_accuracy: 0.8542 - val_loss: 0.0680 - val_binary_accuracy: 0.9132 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0868 - binary_accuracy: 0.8553 - val_loss: 0.0679 - val_binary_accuracy: 0.9115 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0859 - binary_accuracy: 0.8579 - val_loss: 0.0700 - val_binary_accuracy: 0.9094 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0847 - binary_accuracy: 0.8583 - val_loss: 0.0665 - val_binary_accuracy: 0.9132 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0835 - binary_accuracy: 0.8613 - val_loss: 0.0728 - val_binary_accuracy: 0.9048 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0834 - binary_accuracy: 0.8623 - val_loss: 0.0674 - val_binary_accuracy: 0.9129 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0828 - binary_accuracy: 0.8635 - val_loss: 0.0733 - val_binary_accuracy: 0.9073 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      2698\n",
            "           1       0.34      0.61      0.44       171\n",
            "\n",
            "    accuracy                           0.91      2869\n",
            "   macro avg       0.66      0.77      0.69      2869\n",
            "weighted avg       0.94      0.91      0.92      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2499  199]\n",
            " [  67  104]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 28s 398ms/step - loss: 0.1464 - binary_accuracy: 0.7281 - val_loss: 0.1652 - val_binary_accuracy: 0.7219 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1252 - binary_accuracy: 0.7979 - val_loss: 0.0942 - val_binary_accuracy: 0.9362 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1184 - binary_accuracy: 0.8087 - val_loss: 0.0724 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.1152 - binary_accuracy: 0.8125 - val_loss: 0.0643 - val_binary_accuracy: 0.9421 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1126 - binary_accuracy: 0.8184 - val_loss: 0.0615 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1108 - binary_accuracy: 0.8154 - val_loss: 0.0619 - val_binary_accuracy: 0.9414 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1082 - binary_accuracy: 0.8224 - val_loss: 0.0600 - val_binary_accuracy: 0.9407 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1061 - binary_accuracy: 0.8238 - val_loss: 0.0612 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1055 - binary_accuracy: 0.8262 - val_loss: 0.0564 - val_binary_accuracy: 0.9411 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.1024 - binary_accuracy: 0.8316 - val_loss: 0.0572 - val_binary_accuracy: 0.9404 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1002 - binary_accuracy: 0.8360 - val_loss: 0.0570 - val_binary_accuracy: 0.9432 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0984 - binary_accuracy: 0.8340 - val_loss: 0.0584 - val_binary_accuracy: 0.9425 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0977 - binary_accuracy: 0.8390 - val_loss: 0.0583 - val_binary_accuracy: 0.9432 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0959 - binary_accuracy: 0.8358 - val_loss: 0.0622 - val_binary_accuracy: 0.9442 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0944 - binary_accuracy: 0.8428 - val_loss: 0.0613 - val_binary_accuracy: 0.9414 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0932 - binary_accuracy: 0.8440 - val_loss: 0.0646 - val_binary_accuracy: 0.9327 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0908 - binary_accuracy: 0.8474 - val_loss: 0.0668 - val_binary_accuracy: 0.9223 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0901 - binary_accuracy: 0.8536 - val_loss: 0.0712 - val_binary_accuracy: 0.9104 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0873 - binary_accuracy: 0.8591 - val_loss: 0.0731 - val_binary_accuracy: 0.9031 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0868 - binary_accuracy: 0.8599 - val_loss: 0.0726 - val_binary_accuracy: 0.9000 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0843 - binary_accuracy: 0.8663 - val_loss: 0.0767 - val_binary_accuracy: 0.8940 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0849 - binary_accuracy: 0.8655 - val_loss: 0.0724 - val_binary_accuracy: 0.8972 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0828 - binary_accuracy: 0.8669 - val_loss: 0.0757 - val_binary_accuracy: 0.8906 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0818 - binary_accuracy: 0.8665 - val_loss: 0.0756 - val_binary_accuracy: 0.8933 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0809 - binary_accuracy: 0.8699 - val_loss: 0.0736 - val_binary_accuracy: 0.8933 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0795 - binary_accuracy: 0.8723 - val_loss: 0.0742 - val_binary_accuracy: 0.8930 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0792 - binary_accuracy: 0.8725 - val_loss: 0.0743 - val_binary_accuracy: 0.8888 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0777 - binary_accuracy: 0.8749 - val_loss: 0.0754 - val_binary_accuracy: 0.8926 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0770 - binary_accuracy: 0.8781 - val_loss: 0.0745 - val_binary_accuracy: 0.8909 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0764 - binary_accuracy: 0.8765 - val_loss: 0.0720 - val_binary_accuracy: 0.8906 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 12ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94      2698\n",
            "           1       0.28      0.51      0.36       171\n",
            "\n",
            "    accuracy                           0.89      2869\n",
            "   macro avg       0.62      0.71      0.65      2869\n",
            "weighted avg       0.93      0.89      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2467  231]\n",
            " [  83   88]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/30\n",
            "11/11 [==============================] - 25s 397ms/step - loss: 0.1508 - binary_accuracy: 0.7113 - val_loss: 0.1620 - val_binary_accuracy: 0.7578 - lr: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 2/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1263 - binary_accuracy: 0.7905 - val_loss: 0.0990 - val_binary_accuracy: 0.9303 - lr: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 3/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1177 - binary_accuracy: 0.8188 - val_loss: 0.0808 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 4/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1150 - binary_accuracy: 0.8194 - val_loss: 0.0721 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 5/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1133 - binary_accuracy: 0.8186 - val_loss: 0.0683 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 6/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1118 - binary_accuracy: 0.8202 - val_loss: 0.0627 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 7/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1090 - binary_accuracy: 0.8242 - val_loss: 0.0611 - val_binary_accuracy: 0.9428 - lr: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 8/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1077 - binary_accuracy: 0.8244 - val_loss: 0.0576 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 9/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1050 - binary_accuracy: 0.8272 - val_loss: 0.0578 - val_binary_accuracy: 0.9432 - lr: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 10/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.1042 - binary_accuracy: 0.8302 - val_loss: 0.0567 - val_binary_accuracy: 0.9425 - lr: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
            "Epoch 11/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1011 - binary_accuracy: 0.8332 - val_loss: 0.0549 - val_binary_accuracy: 0.9425 - lr: 9.5000e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
            "Epoch 12/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1003 - binary_accuracy: 0.8324 - val_loss: 0.0575 - val_binary_accuracy: 0.9414 - lr: 9.0250e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
            "Epoch 13/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0983 - binary_accuracy: 0.8356 - val_loss: 0.0562 - val_binary_accuracy: 0.9442 - lr: 8.5737e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008145062311086804.\n",
            "Epoch 14/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0958 - binary_accuracy: 0.8410 - val_loss: 0.0586 - val_binary_accuracy: 0.9400 - lr: 8.1451e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0007737808919046074.\n",
            "Epoch 15/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0948 - binary_accuracy: 0.8436 - val_loss: 0.0570 - val_binary_accuracy: 0.9394 - lr: 7.7378e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.000735091819660738.\n",
            "Epoch 16/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0923 - binary_accuracy: 0.8452 - val_loss: 0.0576 - val_binary_accuracy: 0.9359 - lr: 7.3509e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0006983372120885178.\n",
            "Epoch 17/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0922 - binary_accuracy: 0.8454 - val_loss: 0.0672 - val_binary_accuracy: 0.9174 - lr: 6.9834e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0006634203542489559.\n",
            "Epoch 18/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0903 - binary_accuracy: 0.8494 - val_loss: 0.0623 - val_binary_accuracy: 0.9233 - lr: 6.6342e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0006302493420662358.\n",
            "Epoch 19/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0879 - binary_accuracy: 0.8518 - val_loss: 0.0626 - val_binary_accuracy: 0.9184 - lr: 6.3025e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.0005987368611386045.\n",
            "Epoch 20/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0888 - binary_accuracy: 0.8510 - val_loss: 0.0673 - val_binary_accuracy: 0.9007 - lr: 5.9874e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0005688000208465382.\n",
            "Epoch 21/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0881 - binary_accuracy: 0.8561 - val_loss: 0.0659 - val_binary_accuracy: 0.9115 - lr: 5.6880e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.0005403600225690752.\n",
            "Epoch 22/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0861 - binary_accuracy: 0.8530 - val_loss: 0.0747 - val_binary_accuracy: 0.8895 - lr: 5.4036e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.0005133419937919825.\n",
            "Epoch 23/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0851 - binary_accuracy: 0.8569 - val_loss: 0.0636 - val_binary_accuracy: 0.8996 - lr: 5.1334e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.0004876748775132.\n",
            "Epoch 24/30\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0828 - binary_accuracy: 0.8619 - val_loss: 0.0781 - val_binary_accuracy: 0.8853 - lr: 4.8767e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00046329112810781223.\n",
            "Epoch 25/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0816 - binary_accuracy: 0.8607 - val_loss: 0.0721 - val_binary_accuracy: 0.8926 - lr: 4.6329e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00044012657308485355.\n",
            "Epoch 26/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0797 - binary_accuracy: 0.8675 - val_loss: 0.0708 - val_binary_accuracy: 0.8919 - lr: 4.4013e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00041812024719547477.\n",
            "Epoch 27/30\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0802 - binary_accuracy: 0.8673 - val_loss: 0.0828 - val_binary_accuracy: 0.8777 - lr: 4.1812e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00039721422654110934.\n",
            "Epoch 28/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0788 - binary_accuracy: 0.8703 - val_loss: 0.0730 - val_binary_accuracy: 0.8885 - lr: 3.9721e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00037735351797891776.\n",
            "Epoch 29/30\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0778 - binary_accuracy: 0.8699 - val_loss: 0.0867 - val_binary_accuracy: 0.8735 - lr: 3.7735e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00035848583793267607.\n",
            "Epoch 30/30\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0777 - binary_accuracy: 0.8727 - val_loss: 0.0753 - val_binary_accuracy: 0.8857 - lr: 3.5849e-04\n",
            "90/90 [==============================] - 3s 11ms/step\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      2698\n",
            "           1       0.31      0.73      0.43       171\n",
            "\n",
            "    accuracy                           0.89      2869\n",
            "   macro avg       0.64      0.81      0.68      2869\n",
            "weighted avg       0.94      0.89      0.91      2869\n",
            "\n",
            "Confusion matrix:\n",
            "[[2416  282]\n",
            " [  46  125]]\n",
            "0.7115966585388602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6844800300421126"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "9aN51bsbKZhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e1c100-79da-454d-a71f-b47e324b9007"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "Xtt = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "Ytt = np.argmax(y_train, axis=1)\n",
        "\n",
        "Xvv = X_valid.reshape((X_valid.shape[0],X_valid.shape[1]*X_valid.shape[2]))\n",
        "Yvv = np.argmax(y_valid, axis=1)"
      ],
      "metadata": {
        "id": "URL1KrVpKZhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9334262809341234"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "xx = SVC()\n",
        "xx.fit(Xtt,Ytt)\n",
        "xx.score(Xvv,Yvv)"
      ],
      "metadata": {
        "id": "1afBkDQDKZhf",
        "outputId": "cbe1d0ee-cfc8-4989-c053-7d68c65f810f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rr = RandomForestClassifier()\n",
        "rr.fit(Xtt,Ytt)"
      ],
      "metadata": {
        "id": "xGWtIyv6KZhg",
        "outputId": "24aa84e9-ec9e-4a8c-c155-b386d5cd0614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "pred = rr.predict(Xvv)"
      ],
      "metadata": {
        "id": "m1E0fxm3KZhh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.80      0.87      2698\n",
            "           1       0.10      0.35      0.16       171\n",
            "\n",
            "    accuracy                           0.78      2869\n",
            "   macro avg       0.53      0.58      0.51      2869\n",
            "weighted avg       0.90      0.78      0.83      2869\n",
            "\n",
            "[[2165  533]\n",
            " [ 111   60]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Yvv,pred))\n",
        "print(confusion_matrix(Yvv,pred))"
      ],
      "metadata": {
        "id": "tSDUmQoqKZhh",
        "outputId": "b43bfba9-715c-4a35-ec33-ddb9f0e09468",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([2698,  171]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "np.unique(Yvv,return_counts=True)"
      ],
      "metadata": {
        "id": "zPUw0wqlKZhi",
        "outputId": "44de7f33-0d4b-452c-eaca-41c39e7c863b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9365756541524459"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "3293/(3293+223)"
      ],
      "metadata": {
        "id": "ZOl2PexwKZhi",
        "outputId": "0e795779-614f-415c-871b-8866cd8cc1c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "KnsMgFFBKZhj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}