# -*- coding: utf-8 -*-
"""All.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oBOY0cKTQWXl-4MTfZRI1k78QvSiImd

# Mount directory
"""

import os
import sys
from google.colab import drive
drive.mount('/content/drive')
os.chdir("/content/drive/MyDrive/COMP0053_Group6-main/Software")

"""# Import packages and utilities"""

from data_utils import load_data, flatten_data
from early_model import stacked_lstm
from model_utils import model_pipeline, plot_history
import numpy as np
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix
from scipy.stats import ttest_ind
from scipy.stats import spearmanr
from keras.models import Model
from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense
from keras.layers import BatchNormalization, Activation
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from keras import Sequential
from keras.layers import LSTM, Dropout, Dense
from visualisation_utils import plot_model_scores,plot_confusion_matrix
def plot_confusion_matrix(y_true, y_pred):
    # Generate confusion matrix data
    matrix = confusion_matrix(y_true, y_pred, normalize='true')

    plt.figure(figsize=(8, 6))
    sns.heatmap(matrix, annot=True, fmt='.2%', cmap='Blues', cbar=False, xticklabels=True, yticklabels=True)

    plt.title('Confusion Matrix (Percentages)')
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

"""# Load dataset"""

train_participant_num = ["C56D", "C93D", "C382D", "C382N", "C544D", "C709N", "C788N", "P113D", "P113N", "P191D", "P191N", "P299D", "P299N", "P300D", "P336D", "P492D", "P492N", "P531N", "P699D", "P699N", "P890N", "P921D", "P921N"]
valid_participant_num = ["C67D", "C202D", "C202N", "C256D", "C256N", "P54D", "P54N", "P342D", "P342N", "P487D", "P487N", "P649N"]

X_train, y_train = load_data(train_participant_num, 'train', downsampling=True, angle_energy=False, augment=False)
X_valid, y_valid = load_data(valid_participant_num, 'validation', downsampling=True)

num_classes = y_train.shape[1]

"""# Define CNN Model"""

def cnn_normal(input_shape):
    """
    Creates a simple 1D CNN model for processing time series data with a dynamic input shape.

    Args:
    input_shape (tuple): The shape of the input data, excluding the batch size.

    Returns:
    keras.engine.training.Model: A 1D CNN model.
    """
    input_data = Input(shape=input_shape)

    # Convolutional layer block 1
    x = Conv1D(32, 3, padding='same')(input_data)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x)

    # Convolutional layer block 2
    x = Conv1D(64, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x)

    # Convolutional layer block 3
    x = Conv1D(128, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x)

    # Flattening the output and adding Dense layers
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(2, activation='softmax')(x)  # Assuming a binary classification task

    model = Model(inputs=input_data, outputs=out)
    return model

"""# CNN

# 70 features fused into 1 modality
"""

model = cnn_normal(input_shape=(180, 70))
y_pred, y_true, H = model_pipeline(model, X_train, y_train, X_valid, y_valid, epoch=30)
plot_confusion_matrix(y_true, y_pred)

"""# 70 features fused into 2 modalities"""

#X,Y,Z Coordinates
X_train_pose = X_train[:, :, 0:66]
X_valid_pose = X_valid[:, :, 0:66]
#sEMG
X_train_sEMG = X_train[:, :, 66:70]
X_valid_sEMG = X_valid[:, :, 66:70]

model_pose = cnn_normal(input_shape=(180, 66))
y_pred_pose, y_true_pose, H_pose = model_pipeline(model_pose, X_train_pose, y_train, X_valid_pose, y_valid, epoch=30)
model_sEMG = cnn_normal(input_shape=(180, 4))
y_pred_sEMG, y_true_sEMG, H_sEMG = model_pipeline(model_sEMG, X_train_sEMG, y_train, X_valid_sEMG, y_valid, epoch=30)

pose_correlations = []
for i in range(X_train_pose.shape[2]):  # 遍历每个特征维度
    repeated_y_train = np.repeat(y_train, X_train_pose.shape[1])
    coef, p_value = spearmanr(X_train_pose[:, :, i].flatten(), repeated_y_train[:X_train_pose[:, :, i].flatten().shape[0]])
    pose_correlations.append((coef, p_value))
# 对于sEMG数据，我们采取相同的方法
sEMG_correlations = []
for i in range(X_train_sEMG.shape[2]):  # 遍历每个特征维度
    repeated_y_train = np.repeat(y_train, X_train_sEMG.shape[1])
    coef, p_value = spearmanr(X_train_sEMG[:, :, i].flatten(), repeated_y_train[:X_train_sEMG[:, :, i].flatten().shape[0]])
    sEMG_correlations.append((coef, p_value))



weight_pose_abs_avg = np.mean(np.abs([coef for coef, p in pose_correlations]))
weight_sEMG_abs_avg = np.mean(np.abs([coef for coef, p in sEMG_correlations]))
# print(weight_pose_abs_avg)
# print(weight_sEMG_abs_avg)

# 权重归一化
total_weight_avg = weight_pose_abs_avg + weight_sEMG_abs_avg
normalized_weight_pose = weight_pose_abs_avg / total_weight_avg
normalized_weight_sEMG = weight_sEMG_abs_avg / total_weight_avg
print(normalized_weight_pose)
print(normalized_weight_sEMG)

weight_pose = normalized_weight_pose
weight_sEMG = normalized_weight_sEMG


final_pred = y_pred_pose * weight_pose + y_pred_sEMG * weight_pose
final_pred = np.round(final_pred).astype(int)


print(classification_report(y_true_pose, final_pred))
print(confusion_matrix(y_true_pose, final_pred))
plot_confusion_matrix(y_true_pose, final_pred)

# /2
weight_pose = 0.5
weight_sEMG = 0.5


final_pred = y_pred_pose * weight_pose + y_pred_sEMG * weight_pose
final_pred = np.round(final_pred).astype(int)


print(classification_report(y_true_pose, final_pred))
print(confusion_matrix(y_true_pose, final_pred))
plot_confusion_matrix(y_true_pose, final_pred)

"""# 70 features fused into 4 modalities"""

X_train_XYZ = X_train[:, :, :66]
X_train_sEMG = X_train[:, :, 66:70]

# Modality groups definition
trunk_indices = [0, 7, 8, 19, 20, 21]
upper_limb_indices = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
lower_limb_indices = [1, 2, 3, 4, 5, 6]
sEMG_indices = list(range(66, 70))

# Function to prepare modality-specific datasets
def prepare_modality_data(X, indices, is_sEMG=False):
    if is_sEMG:
        return X[:, :, indices]
    else:
        all_indices = []
        for i in indices:
            all_indices.extend([i, i+22, i+44])
        return X[:, :, all_indices]

# Define modalities
modalities = {
    "Trunk": trunk_indices,
    "Upper Limb": upper_limb_indices,
    "Lower Limb": lower_limb_indices,
    "sEMG": sEMG_indices
}
predictions = {}

for modality_name, indices in modalities.items():
    is_sEMG = (modality_name == "sEMG")
    X_train_modality = prepare_modality_data(X_train, indices, is_sEMG)
    X_valid_modality = prepare_modality_data(X_valid, indices, is_sEMG)

    input_shape = (X_train_modality.shape[1], X_train_modality.shape[2])
    model = cnn_normal(input_shape)  # Use cnn_normal instead of stacked_lstm
    y_pred_modality, y_true_modality, _ = model_pipeline(model, X_train_modality, y_train, X_valid_modality, y_valid, 30)

    predictions[modality_name] = y_pred_modality

# 初始化相关性列表
correlations_trunk = []
correlations_upper_limb = []
correlations_lower_limb = []

# 分别计算每个模态组的相关性
for group, correlations in [(trunk_indices, correlations_trunk),
                            (upper_limb_indices, correlations_upper_limb),
                            (lower_limb_indices, correlations_lower_limb)]:
    group_correlations = []
    for i in group:
        # 获取每个XYZ坐标的展平后的数组
        X_flat = X_train[:, :, i].flatten()
        Y_flat = X_train[:, :, i+22].flatten()
        Z_flat = X_train[:, :, i+44].flatten()
        # 为了匹配X_flat, Y_flat, Z_flat的长度，我们需要正确地重复y_train
        y_repeated = np.repeat(y_train, X_train.shape[1])
        # 计算相关性
        coef_X, _ = spearmanr(X_flat, y_repeated[:len(X_flat)])
        coef_Y, _ = spearmanr(Y_flat, y_repeated[:len(Y_flat)])
        coef_Z, _ = spearmanr(Z_flat, y_repeated[:len(Z_flat)])
        # 计算平均相关系数
        avg_coef = np.mean([np.abs(coef_X), np.abs(coef_Y), np.abs(coef_Z)])
        group_correlations.append(avg_coef)
    # 计算并保存该模态组的平均相关性
    avg_group_correlation = np.mean(group_correlations)
    correlations.append(avg_group_correlation)

correlations_sEMG = []
for i in range(4):
    sEMG_flat = X_train_sEMG[:, :, i].flatten()
    y_repeated_sEMG = np.repeat(y_train, X_train_sEMG.shape[1])
    coef_sEMG, _ = spearmanr(sEMG_flat, y_repeated_sEMG[:len(sEMG_flat)])
    correlations_sEMG.append(coef_sEMG)

correlation_sEMG = np.mean(np.abs(correlations_sEMG))

all_correlations = []
all_correlations.append(correlations_trunk[0])

all_correlations.append(correlations_upper_limb[0])
all_correlations.append(correlations_lower_limb[0])
all_correlations.append(correlation_sEMG)

print("Correlations for Trunk:", correlations_trunk)
print("Correlations for Upper Limb:", correlations_upper_limb)
print("Correlations for Lower Limb:", correlations_lower_limb)
print("Correlations for sEMG signals:", correlation_sEMG)
print("Correlations for All:", all_correlations)
# Calculate absolute values and normalize to get initial weights
normalized_weights = all_correlations / np.sum(all_correlations)
# Clip weights exceeding the threshold and redistribute if necessary
max_threshold = 0.42
clipped_weights = np.clip(normalized_weights, None, max_threshold)

# Redistribute weights if any were clipped to the threshold
if np.any(clipped_weights == max_threshold):
    # Calculate the total weight to be redistributed among non-clipped weights
    total_redistribute_weight = 1 - np.sum(clipped_weights == max_threshold) * max_threshold
    # Calculate the sum of weights that are less than the threshold (these will be redistributed)
    sum_weights_to_redistribute = np.sum(clipped_weights[clipped_weights < max_threshold])
    # Adjust weights that are below the threshold
    for i, weight in enumerate(clipped_weights):
        if weight < max_threshold:
            clipped_weights[i] = weight / sum_weights_to_redistribute * total_redistribute_weight

print(clipped_weights)

"""

> # Using statistical tool to dynamically distribute weights to distribute weights
"""

predictions_list = [predictions[modality] for modality in modalities.keys()]

weighted_predictions = np.zeros(predictions_list[0].shape)

# Apply the clipped and redistributed weights to the predictions
for i, prediction in enumerate(predictions_list):
    # print(clipped_weights[i])
    weighted_predictions += prediction * clipped_weights[i]
    # weighted_predictions += prediction / 4

# print(weighted_predictions)


final_predictions = np.round(weighted_predictions)

# Convert y_valid to class indices if it's in one-hot encoding
y_valid_indices = np.argmax(y_valid, axis=1)

# Evaluate the combined predictions
accuracy = accuracy_score(y_valid_indices, final_predictions)
classification_report_result = classification_report(y_valid_indices, final_predictions)
confusion_matrix_result = confusion_matrix(y_valid_indices, final_predictions)

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report_result)
print("Confusion Matrix:\n", confusion_matrix_result)
plot_confusion_matrix(y_valid_indices,final_predictions)

"""> # Using average to distribute weights to distribute weights


"""

clipped_weights = [0.25,0.25,0.25,0.25]
predictions_list = [predictions[modality] for modality in modalities.keys()]

weighted_predictions = np.zeros(predictions_list[0].shape)

# Apply the clipped and redistributed weights to the predictions
for i, prediction in enumerate(predictions_list):
    # print(clipped_weights[i])
    weighted_predictions += prediction * clipped_weights[i]
    # weighted_predictions += prediction / 4

# print(weighted_predictions)


final_predictions = np.round(weighted_predictions)

# Convert y_valid to class indices if it's in one-hot encoding
y_valid_indices = np.argmax(y_valid, axis=1)

# Evaluate the combined predictions
accuracy = accuracy_score(y_valid_indices, final_predictions)
classification_report_result = classification_report(y_valid_indices, final_predictions)
confusion_matrix_result = confusion_matrix(y_valid_indices, final_predictions)

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report_result)
print("Confusion Matrix:\n", confusion_matrix_result)
plot_confusion_matrix(y_valid_indices,final_predictions)

"""# Define LSTM"""

# Create a stacked LSTM model with dropout layers
def stacked_lstm(input_shape,num_classes):
    """
    Creates a stacked LSTM model with dropout layers.
    Parameters:
        input_shape (tuple): Shape of the input data.
        num_classes (int): Number of classes for classification.

    Returns:
        keras.Sequential: A sequential model containing stacked LSTM and dropout layers.
    """
    model = Sequential()
    model.add(LSTM(32, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.5))
    model.add(LSTM(32))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    return model

"""# LSTM

# 70 features fused into 1 modality
"""

model = stacked_lstm(input_shape=(180, 70), num_classes=num_classes)
y_pred, y_true, H = model_pipeline(model, X_train, y_train, X_valid, y_valid, epoch=30)
plot_confusion_matrix(y_true, y_pred)

"""# 70 features fused into 2 modalities"""

#X,Y,Z Coordinates
X_train_pose = X_train[:, :, 0:66]
X_valid_pose = X_valid[:, :, 0:66]
#sEMG
X_train_sEMG = X_train[:, :, 66:70]
X_valid_sEMG = X_valid[:, :, 66:70]

model_pose = stacked_lstm(input_shape=(180, 66), num_classes=num_classes)
y_pred_pose, y_true_pose, H_pose = model_pipeline(model_pose, X_train_pose, y_train, X_valid_pose, y_valid, epoch=30)
model_sEMG = stacked_lstm(input_shape=(180, 4), num_classes=num_classes)
y_pred_sEMG, y_true_sEMG, H_sEMG = model_pipeline(model_sEMG, X_train_sEMG, y_train, X_valid_sEMG, y_valid, epoch=30)

pose_correlations = []
for i in range(X_train_pose.shape[2]):  # 遍历每个特征维度
    repeated_y_train = np.repeat(y_train, X_train_pose.shape[1])
    coef, p_value = spearmanr(X_train_pose[:, :, i].flatten(), repeated_y_train[:X_train_pose[:, :, i].flatten().shape[0]])
    pose_correlations.append((coef, p_value))
# 对于sEMG数据，我们采取相同的方法
sEMG_correlations = []
for i in range(X_train_sEMG.shape[2]):  # 遍历每个特征维度
    repeated_y_train = np.repeat(y_train, X_train_sEMG.shape[1])
    coef, p_value = spearmanr(X_train_sEMG[:, :, i].flatten(), repeated_y_train[:X_train_sEMG[:, :, i].flatten().shape[0]])
    sEMG_correlations.append((coef, p_value))



weight_pose_abs_avg = np.mean(np.abs([coef for coef, p in pose_correlations]))
weight_sEMG_abs_avg = np.mean(np.abs([coef for coef, p in sEMG_correlations]))
# print(weight_pose_abs_avg)
# print(weight_sEMG_abs_avg)

# 权重归一化
total_weight_avg = weight_pose_abs_avg + weight_sEMG_abs_avg
normalized_weight_pose = weight_pose_abs_avg / total_weight_avg
normalized_weight_sEMG = weight_sEMG_abs_avg / total_weight_avg
print(normalized_weight_pose)
print(normalized_weight_sEMG)

weight_pose = normalized_weight_pose
weight_sEMG = normalized_weight_sEMG


final_pred = y_pred_pose * weight_pose + y_pred_sEMG * weight_pose
final_pred = np.round(final_pred).astype(int)


print(classification_report(y_true_pose, final_pred))
print(confusion_matrix(y_true_pose, final_pred))
plot_confusion_matrix(y_true_pose, final_pred)

# /2
weight_pose = 0.51
weight_sEMG = 0.59


final_pred = y_pred_pose * weight_pose + y_pred_sEMG * weight_pose
final_pred = np.round(final_pred).astype(int)


print(classification_report(y_true_pose, final_pred))
print(confusion_matrix(y_true_pose, final_pred))
plot_confusion_matrix(y_true_pose, final_pred)

"""# 70 features fused into 4 modalities"""

X_train_XYZ = X_train[:, :, :66]
X_train_sEMG = X_train[:, :, 66:70]

# 定义模态分组
trunk_indices = [0, 7, 8, 19, 20, 21]  # 身体躯干模态索引
upper_limb_indices = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]  # 上肢模态索引
lower_limb_indices = [1, 2, 3, 4, 5, 6]  # 下肢模态索引

# 初始化相关性列表
correlations_trunk = []
correlations_upper_limb = []
correlations_lower_limb = []

# 分别计算每个模态组的相关性
for group, correlations in [(trunk_indices, correlations_trunk),
                            (upper_limb_indices, correlations_upper_limb),
                            (lower_limb_indices, correlations_lower_limb)]:
    group_correlations = []
    for i in group:
        # 获取每个XYZ坐标的展平后的数组
        X_flat = X_train[:, :, i].flatten()
        Y_flat = X_train[:, :, i+22].flatten()
        Z_flat = X_train[:, :, i+44].flatten()
        # 为了匹配X_flat, Y_flat, Z_flat的长度，我们需要正确地重复y_train
        y_repeated = np.repeat(y_train, X_train.shape[1])
        # 计算相关性
        coef_X, _ = spearmanr(X_flat, y_repeated[:len(X_flat)])
        coef_Y, _ = spearmanr(Y_flat, y_repeated[:len(Y_flat)])
        coef_Z, _ = spearmanr(Z_flat, y_repeated[:len(Z_flat)])
        # 计算平均相关系数
        avg_coef = np.mean([np.abs(coef_X), np.abs(coef_Y), np.abs(coef_Z)])
        group_correlations.append(avg_coef)
    # 计算并保存该模态组的平均相关性
    avg_group_correlation = np.mean(group_correlations)
    correlations.append(avg_group_correlation)

correlations_sEMG = []
for i in range(4):
    sEMG_flat = X_train_sEMG[:, :, i].flatten()
    y_repeated_sEMG = np.repeat(y_train, X_train_sEMG.shape[1])
    coef_sEMG, _ = spearmanr(sEMG_flat, y_repeated_sEMG[:len(sEMG_flat)])
    correlations_sEMG.append(coef_sEMG)

correlation_sEMG = np.mean(np.abs(correlations_sEMG))

all_correlations = []
all_correlations.append(correlations_trunk[0])

all_correlations.append(correlations_upper_limb[0])
all_correlations.append(correlations_lower_limb[0])
all_correlations.append(correlation_sEMG)

print("Correlations for Trunk:", correlations_trunk)
print("Correlations for Upper Limb:", correlations_upper_limb)
print("Correlations for Lower Limb:", correlations_lower_limb)
print("Correlations for sEMG signals:", correlation_sEMG)
print("Correlations for All:", all_correlations)
# Calculate absolute values and normalize to get initial weights
normalized_weights = all_correlations / np.sum(all_correlations)

# Clip weights exceeding the threshold and redistribute if necessary
max_threshold = 0.5
clipped_weights = np.clip(normalized_weights, None, max_threshold)

# Redistribute weights if any were clipped to the threshold
if np.any(clipped_weights == max_threshold):
    # Calculate the total weight to be redistributed among non-clipped weights
    total_redistribute_weight = 1 - np.sum(clipped_weights == max_threshold) * max_threshold
    # Calculate the sum of weights that are less than the threshold (these will be redistributed)
    sum_weights_to_redistribute = np.sum(clipped_weights[clipped_weights < max_threshold])
    # Adjust weights that are below the threshold
    for i, weight in enumerate(clipped_weights):
        if weight < max_threshold:
            clipped_weights[i] = weight / sum_weights_to_redistribute * total_redistribute_weight

print(clipped_weights)
# Modality groups definition
trunk_indices = [0, 7, 8, 19, 20, 21]
upper_limb_indices = [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
lower_limb_indices = [1, 2, 3, 4, 5, 6]
sEMG_indices = list(range(66, 70))

# Function to prepare modality-specific datasets
def prepare_modality_data(X, indices, is_sEMG=False):
    if is_sEMG:
        return X[:, :, indices]
    else:
        all_indices = []
        for i in indices:
            all_indices.extend([i, i+22, i+44])
        return X[:, :, all_indices]

# Define modalities
modalities = {
    "Trunk": trunk_indices,
    "Upper Limb": upper_limb_indices,
    "Lower Limb": lower_limb_indices,
    "sEMG": sEMG_indices
}

predictions = {}

for modality_name, indices in modalities.items():
    is_sEMG = (modality_name == "sEMG")
    X_train_modality = prepare_modality_data(X_train, indices, is_sEMG)
    X_valid_modality = prepare_modality_data(X_valid, indices, is_sEMG)

    input_shape = (X_train_modality.shape[1], X_train_modality.shape[2])
    model = stacked_lstm(input_shape, num_classes)
    y_pred_modality, y_true_modality, _ = model_pipeline(model, X_train_modality, y_train, X_valid_modality, y_valid, 30)

    predictions[modality_name] = y_pred_modality

"""

> # Using statistical tool to dynamically distribute weights to distribute weights

"""

predictions_list = [predictions[modality] for modality in modalities.keys()]

weighted_predictions = np.zeros(predictions_list[0].shape)

# Apply the clipped and redistributed weights to the predictions
for i, prediction in enumerate(predictions_list):
    # print(clipped_weights[i])
    weighted_predictions += prediction * clipped_weights[i]
    # weighted_predictions += prediction / 4

# print(weighted_predictions)


final_predictions = np.round(weighted_predictions)

# Convert y_valid to class indices if it's in one-hot encoding
y_valid_indices = np.argmax(y_valid, axis=1)

# Evaluate the combined predictions
accuracy = accuracy_score(y_valid_indices, final_predictions)
classification_report_result = classification_report(y_valid_indices, final_predictions)
confusion_matrix_result = confusion_matrix(y_valid_indices, final_predictions)

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report_result)
print("Confusion Matrix:\n", confusion_matrix_result)
plot_confusion_matrix(y_valid_indices,final_predictions)

"""

>  # Using average to distribute weights to distribute weights

"""

clipped_weights = [0.25,0.25,0.25,0.25]
predictions_list = [predictions[modality] for modality in modalities.keys()]

weighted_predictions = np.zeros(predictions_list[0].shape)

# Apply the clipped and redistributed weights to the predictions
for i, prediction in enumerate(predictions_list):
    # print(clipped_weights[i])
    weighted_predictions += prediction * clipped_weights[i]
    # weighted_predictions += prediction / 4

# print(weighted_predictions)


final_predictions = np.round(weighted_predictions)

# Convert y_valid to class indices if it's in one-hot encoding
y_valid_indices = np.argmax(y_valid, axis=1)

# Evaluate the combined predictions
accuracy = accuracy_score(y_valid_indices, final_predictions)
classification_report_result = classification_report(y_valid_indices, final_predictions)
confusion_matrix_result = confusion_matrix(y_valid_indices, final_predictions)

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report_result)
print("Confusion Matrix:\n", confusion_matrix_result)
plot_confusion_matrix(y_valid_indices,final_predictions)

"""# Results visualization"""

model_scores = {
    'LSTM (1 mod.)': [0.93, 0.52, 0.50, 0.49],
    'CNN (1 mod.)': [0.87, 0.58, 0.64, 0.60],
    'CNN-Attention (1 mod.)': [0.79, 0.55, 0.64, 0.54],
    'LSTM+Stat (2 mod.)': [0.86, 0.62, 0.81, 0.65],
    'CNN+Stat (2 mod.)': [0.91, 0.65, 0.74, 0.68],
    'CNN-Attention+Stat (2 mod.)': [0.87, 0.64, 0.83, 0.67],
    'LSTM+Stat (4 mod.)': [0.86, 0.62, 0.78, 0.65],
    'CNN+Stat (4 mod.)': [0.91, 0.66, 0.75, 0.69],
    'CNN-Attention+Stat (4 mod.)': [0.89, 0.66, 0.85, 0.70],
    'LSTM+Avg (4 mod.)': [0.94, 0.63, 0.52, 0.52],
    'CNN+Avg (4 mod.)': [0.87, 0.58, 0.65, 0.60],
    'CNN-Attention+Avg (4 mod.)': [0.85, 0.59, 0.72, 0.62]
}

plot_model_scores(model_scores)

model_scores = {
    'LSTM (1 mod.)': [0.93, 0.52, 0.50, 0.49],
    'LSTM+Stat (2 mod.)': [0.86, 0.62, 0.81, 0.65],
    'LSTM+Stat (4 mod.)': [0.86, 0.62, 0.78, 0.65],
    'LSTM+Avg (4 mod.)': [0.94, 0.63, 0.52, 0.52],
    'CNN (1 mod.)': [0.87, 0.58, 0.64, 0.60],
    'CNN+Stat (2 mod.)': [0.91, 0.65, 0.74, 0.68],
    'CNN+Stat (4 mod.)': [0.91, 0.66, 0.75, 0.69],
    'CNN+Avg (4 mod.)': [0.87, 0.58, 0.65, 0.60],
    'CNN-Attention (1 mod.)': [0.79, 0.55, 0.64, 0.54],
    'CNN-Attention+Stat (2 mod.)': [0.87, 0.64, 0.83, 0.67],
    'CNN-Attention+Stat (4 mod.)': [0.89, 0.66, 0.85, 0.70],
    'CNN-Attention+Avg (4 mod.)': [0.85, 0.59, 0.72, 0.62]
}
plot_model_scores(model_scores)